# 上海餐厅数据分析



```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.style import use
use('seaborn')

# 中文乱码
plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签
plt.rcParams['axes.unicode_minus']=False #用来正常显示负号

from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import LinearSVR
from sklearn.neighbors import KNeighborsRegressor

from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.decomposition import PCA
```


```python
# 删除空变量
data = pd.read_csv('C:/pwork/上海餐饮数据.csv')
data = data.drop(['Unnamed: 10', 'Unnamed: 11', '城市'], axis=1)
```


```python
# data.shape
# data.isnull().sum()
# data.columns
# data01.describe()
# 获取有点评的餐厅
data01 = data[(data['点评数'] != 0) & (data['口味'] != 0) & (data['人均消费'] != 0) & (data['Lng'] > 120)]
# (data['点评数'] != 0) & (data['口味'] != 0)
```


```python
# data01.isnull().sum()
data_dropna = data01.dropna()

data02 = pd.get_dummies(data_dropna)
data02.describe()
```

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>点评数</th>
      <th>口味</th>
      <th>环境</th>
      <th>服务</th>
      <th>人均消费</th>
      <th>Lng</th>
      <th>Lat</th>
      <th>类别_亚菜</th>
      <th>类别_助餐</th>
      <th>类别_北菜</th>
      <th>...</th>
      <th>行政区_ 松江区</th>
      <th>行政区_ 浦东新区</th>
      <th>行政区_ 虹口区</th>
      <th>行政区_ 金山区</th>
      <th>行政区_ 长宁区</th>
      <th>行政区_ 闵行区</th>
      <th>行政区_ 闸北区</th>
      <th>行政区_ 青浦区</th>
      <th>行政区_ 静安区</th>
      <th>行政区_ 黄浦区</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>...</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
      <td>54875.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>711.386879</td>
      <td>7.589053</td>
      <td>7.570172</td>
      <td>7.532326</td>
      <td>72.664601</td>
      <td>121.431527</td>
      <td>31.218687</td>
      <td>0.008674</td>
      <td>0.011098</td>
      <td>0.014579</td>
      <td>...</td>
      <td>0.055472</td>
      <td>0.154169</td>
      <td>0.045631</td>
      <td>0.000036</td>
      <td>0.073057</td>
      <td>0.102688</td>
      <td>0.045194</td>
      <td>0.030961</td>
      <td>0.060173</td>
      <td>0.057057</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1844.254094</td>
      <td>0.615022</td>
      <td>0.704133</td>
      <td>0.671882</td>
      <td>106.711592</td>
      <td>0.111255</td>
      <td>0.080592</td>
      <td>0.092732</td>
      <td>0.104762</td>
      <td>0.119860</td>
      <td>...</td>
      <td>0.228900</td>
      <td>0.361114</td>
      <td>0.208685</td>
      <td>0.006037</td>
      <td>0.260232</td>
      <td>0.303554</td>
      <td>0.207730</td>
      <td>0.173214</td>
      <td>0.237809</td>
      <td>0.231954</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.000000</td>
      <td>4.000000</td>
      <td>3.800000</td>
      <td>4.200000</td>
      <td>1.000000</td>
      <td>120.879630</td>
      <td>30.733726</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>34.000000</td>
      <td>7.100000</td>
      <td>7.100000</td>
      <td>7.100000</td>
      <td>24.000000</td>
      <td>121.387594</td>
      <td>31.186274</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>129.000000</td>
      <td>7.500000</td>
      <td>7.500000</td>
      <td>7.400000</td>
      <td>45.000000</td>
      <td>121.442901</td>
      <td>31.225007</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>553.000000</td>
      <td>8.000000</td>
      <td>8.000000</td>
      <td>8.000000</td>
      <td>87.000000</td>
      <td>121.486729</td>
      <td>31.262165</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>38643.000000</td>
      <td>9.400000</td>
      <td>9.400000</td>
      <td>9.500000</td>
      <td>6309.000000</td>
      <td>121.967860</td>
      <td>31.721867</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>...</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 51 columns</p>
</div>




```python
data010.to_csv('shanghai.csv', encoding='utf_8_sig')
```

# 前处理

## 1. 因变量


```python
sns.distplot(np.log(data02['人均消费']));plt.show()
sns.distplot(data02['口味']);plt.show()
```


![png](%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_files/%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_7_0.png)



![png](%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_files/%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_7_1.png)


## 2. 类别变量


```python
my_order = data01.groupby(by=["类别"])["口味"].median().sort_values(ascending=False).index
f, ax = plt.subplots(figsize=(18, 10))
fig = sns.boxplot(x='类别', y='口味', data=data01, order=my_order)
# fig.axis(ymin=0, ymax=800000);
```


![png](%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_files/%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_9_0.png)



```python
my_order = data01.groupby(by=["行政区"])["口味"].median().sort_values(ascending=False).index

f, ax = plt.subplots(figsize=(18, 10))
fig = sns.boxplot(x='行政区', y='口味', data=data01, order=my_order)
# fig.axis(ymin=0, ymax=800000);
```


![png](%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_files/%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_10_0.png)


## 3. 相关性


```python
f, ax = plt.subplots(figsize=(12, 9))
hm = sns.heatmap(data01.corr(), cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 20},
                )
```


![png](%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_files/%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_12_0.png)



```python
k = 15
cols = data02.corr()['口味'].sort_values(ascending=False).index[:k]

f, ax = plt.subplots(figsize=(15, 15))
hm = sns.heatmap(data02[cols].corr(), cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 15},
                )
```


![png](%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_files/%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_13_0.png)



```python
sns.pairplot(data01, size=2.5)
```

    c:\users\13631\appdata\local\programs\python\python38\lib\site-packages\seaborn\axisgrid.py:2071: UserWarning: The `size` parameter has been renamed to `height`; please update your code.
      warnings.warn(msg, UserWarning)





    <seaborn.axisgrid.PairGrid at 0x25343352850>




![png](%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_files/%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_14_2.png)


## 4. 地图


```python
plt.figure(figsize=(16,9),dpi=300)
plt.grid(linestyle='--')

plt.xlabel('经度')
plt.ylabel('纬度')
cc = data01.口味
clist = [i for i in cc]
plt.scatter(data01.Lng,data01.Lat,c=clist,cmap='Reds',s=50,alpha=0.5)
```




    <matplotlib.collections.PathCollection at 0x2534c7a73a0>




![png](%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_files/%E4%B8%8A%E6%B5%B7%E9%A4%90%E5%8E%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_16_1.png)


# 模型预测

## 1. 指标函数设定


```python
y1 = data02['口味']
y2 = np.log(data02['人均消费'])
X = data02.drop(['人均消费', '口味'], axis=1)
```


```python
def get_best_model_and_accuracy(model, params, X, y, n_jobs=None, verbose=0):
    grid = GridSearchCV(model, params, error_score=0.,
    n_jobs=n_jobs, verbose=verbose)# 如果报错，结果是0
    grid.fit(X, y)
    # 经典的性能指标
    print("Best Accuracy: {}".format(grid.best_score_))
    # 得到最佳准确率的最佳参数
    print("Best Parameters: {}".format(grid.best_params_)) # 拟合的平均时间(秒)
    print("Average Time to Fit (s):{}".format(round(grid.cv_results_['mean_fit_time'].mean(), 3))) # 预测的平均时间(秒)
    # 从该指标可以看出模型在真实世界的性能
    print("Average Time to Score (s):{}".format(round(grid.cv_results_['mean_score_time'].mean(), 3)))

```

## 2. 模型比较

树模型最优


```python
# tree
tree = DecisionTreeRegressor(random_state=42)
tree_params = {
    'max_depth':[None, 5, 7, 9, 11]
}
for y in [y1, y2]:
    get_best_model_and_accuracy(tree, tree_params, X, y)
```

    Best Accuracy: 0.8279216089887548
    Best Parameters: {'max_depth': 7}
    Average Time to Fit (s):0.682
    Average Time to Score (s):0.015
    Best Accuracy: 0.564797104603786
    Best Parameters: {'max_depth': 7}
    Average Time to Fit (s):0.542
    Average Time to Score (s):0.015



```python
lr = LinearRegression()
lr_params = {'fit_intercept': [True, False]}
for y in [y1, y2]:
    # print(y.columns)
    get_best_model_and_accuracy(lr, lr_params, X, y)
```

    Best Accuracy: 0.8162416929674883
    Best Parameters: {'fit_intercept': True}
    Average Time to Fit (s):0.223
    Average Time to Score (s):0.014
    Best Accuracy: 0.5734112903869907
    Best Parameters: {'fit_intercept': True}
    Average Time to Fit (s):0.271
    Average Time to Score (s):0.019


### 2.1 最优树模型的有用特征重要性


```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y1, random_state=42)
tree = DecisionTreeRegressor(random_state=42, max_depth=7)
tree.fit(X_train, y_train)
tree.score(X_test, y_test)
```




    0.8366493550658072




```python
pd.DataFrame(tree.feature_importances_, X.columns).sort_values(by=0, ascending=False).head(15)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>服务</th>
      <td>0.954290</td>
    </tr>
    <tr>
      <th>点评数</th>
      <td>0.030416</td>
    </tr>
    <tr>
      <th>环境</th>
      <td>0.007391</td>
    </tr>
    <tr>
      <th>类别_助餐</th>
      <td>0.003223</td>
    </tr>
    <tr>
      <th>类别_火锅</th>
      <td>0.001582</td>
    </tr>
    <tr>
      <th>Lng</th>
      <td>0.000848</td>
    </tr>
    <tr>
      <th>类别_浙菜</th>
      <td>0.000508</td>
    </tr>
    <tr>
      <th>类别_西餐</th>
      <td>0.000458</td>
    </tr>
    <tr>
      <th>Lat</th>
      <td>0.000275</td>
    </tr>
    <tr>
      <th>类别_啡厅</th>
      <td>0.000256</td>
    </tr>
    <tr>
      <th>类别_海鲜</th>
      <td>0.000246</td>
    </tr>
    <tr>
      <th>行政区_ 黄浦区</th>
      <td>0.000215</td>
    </tr>
    <tr>
      <th>类别_蟹宴</th>
      <td>0.000151</td>
    </tr>
    <tr>
      <th>类别_湾菜</th>
      <td>0.000102</td>
    </tr>
    <tr>
      <th>类别_素菜</th>
      <td>0.000040</td>
    </tr>
  </tbody>
</table>
</div>




```python
X.columns
```




    Index(['点评数', '环境', '服务', 'Lng', 'Lat', '类别_亚菜', '类别_助餐', '类别_北菜', '类别_午茶',
           '类别_南菜', '类别_啡厅', '类别_川菜', '类别_州菜', '类别_常菜', '类别_快餐', '类别_料理', '类别_本菜',
           '类别_浙菜', '类别_海鲜', '类别_湘菜', '类别_湾菜', '类别_火锅', '类别_烧烤', '类别_甜点', '类别_疆菜',
           '类别_粤菜', '类别_素菜', '类别_美食', '类别_蟹宴', '类别_西菜', '类别_西餐', '类别_面馆', '类别_龙虾',
           '行政区_ 卢湾区', '行政区_ 嘉定区', '行政区_ 宝山区', '行政区_ 徐汇区', '行政区_ 普陀区', '行政区_ 杨浦区',
           '行政区_ 松江区', '行政区_ 浦东新区', '行政区_ 虹口区', '行政区_ 金山区', '行政区_ 长宁区', '行政区_ 闵行区',
           '行政区_ 闸北区', '行政区_ 青浦区', '行政区_ 静安区', '行政区_ 黄浦区'],
          dtype='object')



### 2.2 类别对口味的影响


```python
data02[['口味', '类别_助餐', '类别_火锅']].corr()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>口味</th>
      <th>类别_助餐</th>
      <th>类别_火锅</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>口味</th>
      <td>1.000000</td>
      <td>-0.033468</td>
      <td>0.146611</td>
    </tr>
    <tr>
      <th>类别_助餐</th>
      <td>-0.033468</td>
      <td>1.000000</td>
      <td>-0.026505</td>
    </tr>
    <tr>
      <th>类别_火锅</th>
      <td>0.146611</td>
      <td>-0.026505</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>



## 3. 特征工程

### 3.1 标准化管道


```python
tree = DecisionTreeRegressor(random_state=42)

tree_pipe = Pipeline([
    ('sc', StandardScaler()),
    ('tree', tree)
])
tree_pipe_params = {
    'tree__max_depth':[None, 1, 3, 5, 7, 9, 11]
}
for y in [y1, y2]:
    get_best_model_and_accuracy(tree_pipe, tree_pipe_params, X, y)
```

    Best Accuracy: 0.8279350617309706
    Best Parameters: {'tree__max_depth': 7}
    Average Time to Fit (s):0.764
    Average Time to Score (s):0.024
    Best Accuracy: 0.5648055115074962
    Best Parameters: {'tree__max_depth': 7}
    Average Time to Fit (s):0.61
    Average Time to Score (s):0.022



### 3.2 加入交互项和平方项


```python
tree = DecisionTreeRegressor(random_state=42)

tree_pipe = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('sc', StandardScaler()),
    ('tree', tree)
])
tree_pipe_params = {
    'tree__max_depth':[None, 5, 7, 9, 11]
}
for y in [y1, y2]:
    get_best_model_and_accuracy(tree_pipe, tree_pipe_params, X, y)
```

    Best Accuracy: 0.8245398000072933
    Best Parameters: {'tree__max_depth': 7}
    Average Time to Fit (s):20.351
    Average Time to Score (s):0.663





### 3.3 特征选择

#### 3.3.1 基于统计的特征选择


```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from copy import deepcopy

tree = DecisionTreeRegressor(random_state=42)
k_best = SelectKBest(f_classif)

tree_select = Pipeline([
#     ('poly', PolynomialFeatures(degree=2)),
    ('sc', StandardScaler()),
    ('select', k_best),
    ('tree', tree)
])

tree_select_params = {
    'tree__max_depth':[None, 5, 7, 9, 11],
    "select__k": [5, 7, 10, 20, 30]
}
# for y in [y1, y2]:
get_best_model_and_accuracy(tree_select, tree_select_params, X, y1)
```


    Best Accuracy: 0.8286881153193469
    Best Parameters: {'select__k': 30, 'tree__max_depth': 7}
    Average Time to Fit (s):0.607
    Average Time to Score (s):0.027


#### 3.3.2 基于模型的特征选择


```python
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectFromModel

select = SelectFromModel(DecisionTreeRegressor())

select_from_pipe = Pipeline([
    ('sc', StandardScaler()),
    ("select", select),
    ("tree", tree)
])

select_from_pipe_params = deepcopy(tree_pipe_params)
select_from_pipe_params.update({
    "select__threshold": [.01, .05, .1, .2, .25, .3, .4, .5, .6, "mean", "median", "2.*mean"],
    'select__estimator__max_depth': [None, 1, 3, 5, 7, 9],
})

print(select_from_pipe_params)
get_best_model_and_accuracy(select_from_pipe, select_from_pipe_params, X, y1)
```

    {'tree__max_depth': [None, 5, 7, 9, 11], 'select__threshold': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 'mean', 'median', '2.*mean'], 'select__estimator__max_depth': [None, 1, 3, 5, 7, 9]}
    Best Accuracy: 0.8279350617309706
    Best Parameters: {'select__estimator__max_depth': 1, 'select__threshold': 'median', 'tree__max_depth': 7}
    Average Time to Fit (s):0.721
    Average Time to Score (s):0.021


#### 3.3.3 交互项和特征选择的合并


```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from copy import deepcopy

tree = DecisionTreeRegressor(random_state=42)
k_best = SelectKBest(f_classif)

tree_select = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('sc', StandardScaler()),
    ('select', k_best),
    ('tree', tree)
])

tree_select_params = {
    'tree__max_depth':[None, 5, 7, 9, 11],
    "select__k": [5, 7, 10, 20, 30]
}
# for y in [y1, y2]:
get_best_model_and_accuracy(tree_select, tree_select_params, X, y1)
```




```python
# from sklearn.pipeline import Pipeline
# from sklearn.feature_selection import SelectFromModel

# select = SelectFromModel(DecisionTreeRegressor())

pca_pipe = Pipeline([
    ('sc', StandardScaler()),
    ('pca', PCA()),
    ('tree', tree)
])
pca_pipe_params = deepcopy(tree_pipe_params)
pca_pipe_params.update({"pca__n_components": [3, 5, 7, 9]})
get_best_model_and_accuracy(pca_pipe, pca_pipe_params, X, y1)
```

    Best Accuracy: 0.633572898867724
    Best Parameters: {'pca__n_components': 5, 'tree__max_depth': 7}
    Average Time to Fit (s):0.787
    Average Time to Score (s):0.02


