# 链家网二手房数据爬虫

date：2020-06-16

源代码：[代码](scrape\链家网二手房数据爬虫.md)

最终数据：[链家网二手房数据](http://myeconomics.cn/geek/pyfile/ershoufang.csv)



## 1. 网页分析

最近教授发了一个课外作业，搜集一下中国房屋数据来做数据分析。正好趁这个机会更新一下博客，爬一下房地产房屋数据。这次我选择的是链家网，也许因为他们经常做广告。先看看他们的网页是怎么样的吧。我随便打开了一个区域：[https://bj.lianjia.com/ershoufang/](https://bj.lianjia.com/ershoufang/)

![image-20200616211730133](https://github.com/fanzhuanjun/fanzhuanjun.github.io/raw/master/myimages/image-20200616211730133.png)

可以发现有很多页，每一页有30个房屋的信息。虽然每页上面也有一些房屋信息，但信息还是不够多，我们需要打开内页去爬才行。打开每个内页，它是这样的。

![image-20200616212046781](https://github.com/fanzhuanjun/fanzhuanjun.github.io/raw/master/myimages/image-20200616212046781.png)

鉴于此，我们大致上可以分为 3 个大的函数：`getPage` 函数、`getLinks` 函数和 `parse` 函数。第一个 `getPage` 主要是获取网页并解析网页，这个是网站爬虫的几乎必须的一个函数，建议收藏起来，不用每次重新写一遍。第二个 `getLinks` 是获取外链的函数，因为我们刚刚看到外页信息不足，所以我们需要访问内页信息，访问内页信息之前就需要获取内页网址。最后一个 `parse` 函数当然是定位信息并保存至文件咯。

## 2. 自定义函数

首先我们先安装一下必备的包吧。我们需要的包有`requests`、`bs4`、`pandas`、`lxml`、`html5lib`。

```
>pip install requests bs4 pandas lxml html5lib
```

- `getPage` 函数

接下来先写一下 `getPage` 函数吧

```python
def getPage(url):
    headers = {
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',
    }
    try:
        req = requests.get(url=url, headers = headers)
        bs = BeautifulSoup(req.text, 'html.parser')
        return bs
    except Exception as e:
        print('getPage error', e)
```

这个函数比较简单。

- `getLinks` 函数

获取链接函数的输入参数为页码数。其实我们点击下一页会发现，改变的只有页码数，所以我索性就只拿页码数作为参数，当然你也可以用每页的网址来作为参数，结果都是一样的。先贴一下代码

```python
def getLinks(offset):
    url = f'https://bj.lianjia.com/ershoufang/pg{offset}/'
    links = []
    try:
        bs = getPage(url)
        infos = bs.findAll('a', {'class': 'noresultRecommend img LOGCLICKDATA'})
        for info in infos:    
            links.append(info.attrs['href'])
        return links
    except Exception as e:
        print('getLink error', e)
```

在这里我用 `bs.findAll()` 来定位网页。然后获取每一个标签的 `href` 属性，建立一个空白列表并 `.append()` 来把页码添加至 links 列表里。

- `parse()` 函数

由于爬取的信息实在很多，在这里简要说说我爬取的信息是如下的两个大的区域。

![image-20200616230024355](https://github.com/fanzhuanjun/fanzhuanjun.github.io/raw/master/myimages/image-20200616230024355.png)

代码如下。这里有很多代码小细节是值得好好思考的。

```python
def parse(bs):
    if bs:
        try:
            h1 = bs.h1.text
            unitprice = bs.find('span', {'class': 'unitPriceValue'}).text
            price = bs.find('span', {'class': 'total'}).text + bs.find('span', {'class': 'unit'}).text
            room_mainInfo = bs.find('div', {'class': 'houseInfo'}).find('div', {'class': 'room'}).find('div', {'class': 'mainInfo'}).text
            room_subInfo = bs.find('div', {'class': 'houseInfo'}).find('div', {'class': 'room'}).find('div', {'class': 'subInfo'}).text
            type_mainInfo = bs.find('div', {'class': 'houseInfo'}).find('div', {'class': 'type'}).find('div', {'class': 'mainInfo'}).text
            type_subInfo = bs.find('div', {'class': 'houseInfo'}).find('div', {'class': 'type'}).find('div', {'class': 'subInfo'}).text
            # type_mainInfo, type_subInfo
            area_mainInfo = bs.find('div', {'class': 'houseInfo'}).find('div', {'class': 'area'}).find('div', {'class': 'mainInfo'}).text
            area_subInfo = bs.find('div', {'class': 'houseInfo'}).find('div', {'class': 'area'}).find('div', {'class': 'subInfo'}).text
            # area_mainInfo,area_subInfo
            communityName = bs.find('div', {'class': 'communityName'}).find('a', {'class': 'info'}).text
            areaName = bs.find('div', {'class': 'areaName'}).text.split('\xa0')
            baseinfo = bs.find('div', {'class': 'base'}).findAll('li')
            content = [info.text for info in baseinfo]
            transactioninfo = bs.find('div', {'class': 'transaction'}).findAll('li')
            transaction = [validateTitle(info.text) for info in transactioninfo]
            b = [
                h1, price, unitprice,
                room_mainInfo, room_subInfo,
                type_mainInfo, type_subInfo,
                area_mainInfo, area_subInfo,
                communityName, *areaName,
            ]
            a = [b, content, transaction]
            result = list(chain(*a))
            write_result(result)
            print(result)
            print('---------------')
        except Exception as e:
            print("parse error!", e)
```



## 3. 代码展示

这是所有的代码。除了三个主要的代码之外，还有 `validateTitle` 函数主要是用于正则化文本，因为有些字符串有空格或者换行符之类的。 `write_result` 函数用于将结果写进 csv 文件中。

对了，这里还运用了多进程。其实我也不太懂，但是多进程就那么几句话，记住是有好处的，可以大大加快你的爬虫速度。`pool = Pool(3)` 这里 3 指的是 3 个进程。最好不要设置的太高，因为如果网站发现同一时间内有太多次访问会屏蔽 ip 的。链家网就是这样，我最开始开了 10 进程，然后就鸡鸡了反正。有些网站可能几个小时就恢复了，有些网站要好几天，也有一些网站会永久屏蔽你的 ip ，所以慎重慎重。

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
from itertools import chain
import re
import csv
from multiprocessing import Pool

def getPage(url):
    headers = {
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',
    }
    try:
        req = requests.get(url=url, headers = headers)
        bs = BeautifulSoup(req.text, 'html.parser')
        return bs
    except Exception as e:
        print('getPage error', e)

def getLinks(offset):
    url = f'https://bj.lianjia.com/ershoufang/pg{offset}/'
    links = []
    try:
        bs = getPage(url)
        infos = bs.findAll('a', {'class': 'noresultRecommend img LOGCLICKDATA'})
        for info in infos:    
            links.append(info.attrs['href'])
        return links
    except Exception as e:
        print('getLink error', e)

def validateTitle(title):
    rstr = r"[\n ]"  # '/ \ : * ? " < > |'
    new_title = re.sub(rstr, "", title)  # 替换为下划线
    return new_title

def write_result(result):
    if result:
        with open('c:/pwork/ershoufang.csv', 'a') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(result)

def parse(bs):
    if bs:
        try:
            h1 = bs.h1.text
            unitprice = bs.find('span', {'class': 'unitPriceValue'}).text
            price = bs.find('span', {'class': 'total'}).text + bs.find('span', {'class': 'unit'}).text
            room_mainInfo = bs.find('div', {'class': 'houseInfo'}).find('div', {'class': 'room'}).find('div', {'class': 'mainInfo'}).text
            room_subInfo = bs.find('div', {'class': 'houseInfo'}).find('div', {'class': 'room'}).find('div', {'class': 'subInfo'}).text
            type_mainInfo = bs.find('div', {'class': 'houseInfo'}).find('div', {'class': 'type'}).find('div', {'class': 'mainInfo'}).text
            type_subInfo = bs.find('div', {'class': 'houseInfo'}).find('div', {'class': 'type'}).find('div', {'class': 'subInfo'}).text
            # type_mainInfo, type_subInfo
            area_mainInfo = bs.find('div', {'class': 'houseInfo'}).find('div', {'class': 'area'}).find('div', {'class': 'mainInfo'}).text
            area_subInfo = bs.find('div', {'class': 'houseInfo'}).find('div', {'class': 'area'}).find('div', {'class': 'subInfo'}).text
            # area_mainInfo,area_subInfo
            communityName = bs.find('div', {'class': 'communityName'}).find('a', {'class': 'info'}).text
            areaName = bs.find('div', {'class': 'areaName'}).text.split('\xa0')
            baseinfo = bs.find('div', {'class': 'base'}).findAll('li')
            content = [info.text for info in baseinfo]
            transactioninfo = bs.find('div', {'class': 'transaction'}).findAll('li')
            transaction = [validateTitle(info.text) for info in transactioninfo]
            b = [
                h1, price, unitprice,
                room_mainInfo, room_subInfo,
                type_mainInfo, type_subInfo,
                area_mainInfo, area_subInfo,
                communityName, *areaName,
            ]
            a = [b, content, transaction]
            result = list(chain(*a))
            write_result(result)
            print(result)
            print('---------------')
        except Exception as e:
            print("parse error!", e)

startpage = 1
endpage = 100

def main(link):
    bs = getPage(link)
    parse(bs)

if __name__ == "__main__":
    pool = Pool(3)
    sumLinks = []
    for page in range(startpage, endpage+1):
        links = getLinks(page)
        print(links)
        sumLinks.extend(getLinks(page))
    pool.map(main, sumLinks)
    pool.close()
    pool.join()
    print('done!')
```

好了，如果你成功运行的话，你会发现，打开来是这个样子的。当然我的数据已经经过清洗，如果你想知道如何清洗，我可能不会告诉你，你去问问别人吧，哈哈哈。就到这里吧。

![image-20200616231108113](https://github.com/fanzhuanjun/fanzhuanjun.github.io/raw/master/myimages/image-20200616231108113.png)