# 数据爬虫实战(wunderground.com)

日期：2020-06-14

源码地址：[代码](https://github.com/fanzhuanjun/geek/tree/master/pyfile/weather.py)

我认为网络爬虫是社会科学的学者学习计算机技术时最重要的一项技能，因为有很多数据不能直接获得，所以我们只能通过爬虫获得。今天我们爬取一个全球天气数据网站：[https://www.wunderground.com/](https://www.wunderground.com/)。之所以爬取这个网站，根由N年前大学毕业的时候，好友小花让我帮他做一个毕业论文的分析，研究天气与股票市场的关系。当时并不懂 python，他后来去某宝上找人爬了这份数据，所以我今天想把这坑给填了。因为当时用的是上海的天气数据，**所以这次我们也来爬取上海2020年1月至6月这半年的天气数据**。



## 1. 网站分析

我们先打开网页：[https://www.wunderground.com/history/daily/ZSPD/date](https://www.wunderground.com/history/daily/ZSPD/date)。它是这个样子的。我写这篇文章时间是2020年6月14日，这里显示的就是当天的天气数据。我们也可以通过改变时间来查阅不同日期的天气数据。

<img src="https://github.com/fanzhuanjun/fanzhuanjun.github.io/raw/master/myimages/image-20200614233207355.png" alt="image-20200614233207355"  />

把页面拉到下面，我们可以发现有一个表格，名叫**Daily Observations** ，这是上海一天的天气数据，每半小时记录一次，一天总共有48条样本。这个就是我们想要获取的数据。

<img src="https://github.com/fanzhuanjun/fanzhuanjun.github.io/raw/master/myimages/image-20200614233640989.png" alt="image-20200614233640989"  />

懂一些网页开发的朋友可能能看出来，这个网页的表格是通过JavaScript渲染得到的，所以如果直接用 `requests` 的 `get` 这个网址，会发现获取的网页并不包含这个表格。这就是这个网站爬虫的难点。

<img src="https://github.com/fanzhuanjun/fanzhuanjun.github.io/raw/master/myimages/image-20200614234007681.png" alt="image-20200614234007681" />

重新刷新一下网页，我们可以清楚看到，在网页没加载出来之前，**Daily Observations**下面显示的是 “No Data Recorded”。我们通过代码实验一下：

```python
import requests
req = requests.get('https://www.wunderground.com/history/daily/ZSPD/date')
"No Data Recorded" in req.text
```

结果显示为：>>> True

证明了刚才的说法。

这时候我们应该去观察网页的源代码，我们点击 `Network`，并选择 `XHR` 。

<img src="https://github.com/fanzhuanjun/fanzhuanjun.github.io/raw/master/myimages/image-20200614234635625.png" alt="image-20200614234635625" />

如果你比较细心的话，可以发现里面 `XHR` 里面就有我们想要的数据，就是上图中的那一条。我们按一下右边的

<img src="https://github.com/fanzhuanjun/fanzhuanjun.github.io/raw/master/myimages/image-20200614234822854.png" alt="image-20200614234822854" />

可以发现它是以 `json` 格式来展示的，点击下拉栏 `observations` 可以发现，这正是我们想要的数据。所以我们直接 `get` 这个网页即可获取我们想要的数据了。



## 2. 一些细节

我们在思考代码如何写之前，应该想到等一下可能会碰到的一些问题：

1. （这是一条建议）我们应该写一个函数 `getData`，参数为 `startdate`，解析并获取 `json` 数据。
2. 如何将 `json` 数据转化为表格模式？

`json` 数据的用法和 python 的字典差不多相同。我们可以用 `pandas` 库的 `DataFrame` 函数来获得 `pandas` 表格。

3. 我们发现表格内少了时间这一个变量？

我们将添加 `date` 变量，将日期写入。

4. 如何合并数据？

我们用一个简单的方式，同样使用 `pandas` 库的 `concat` 函数来合并多个表格，同时辅助使用 `map` 函数。

5. 如何保存数据

我们用 `pandas.to_csv(path)` 将其保存至本地硬盘中，保存为 `csv` 格式。



## 3. 代码示例

运用库：pandas 和 requests。

```
pip install requests pandas
```

下面是主要的代码。**记得更改保存的地址！！！！**

```python
import requests
import pandas as pd
from urllib.parse import urlencode

def getData(startdate):
    url = 'https://api.weather.com/v1/location/ZSPD:9:CN/observations/historical.json?apiKey=6532d6454b8aa370768e63d6ba5a832e&units=e&startDate=' + startdate
    headers = {
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',
    }
    try:
        req = requests.get(url, headers=headers)
        data = pd.DataFrame(req.json()['observations'])
        colName = data.columns.tolist()
        colName.insert(0, 'date')
        data = data.reindex(columns=colName)
        data['date'] = startdate
        return data        
    except Exception as e:
        print(e)

startdate = '20200101'
enddate = '20200601'


def main():
    dateRange = pd.date_range(start=startdate, end=enddate, freq='D')
    date = [date for date in dateRange.strftime('%Y%m%d')]
    mapList = map(getData, date)
    df = pd.concat(mapList, axis=0)
    df.to_csv('c:/pwork/weather.csv') #记得更改保存的地址！！！！
    print(f'done!总列数为{df.shape}')

if __name__ == "__main__":
    main()
```

这时候在目标文件夹就会出现一个新的文件 `weather.csv` ，打开来看是这样的。

<img src="https://github.com/fanzhuanjun/fanzhuanjun.github.io/raw/master/myimages/image-20200615083056844.png" alt="image-20200615083056844" />



## 4. 尾声

好了，今天的爬虫就到这里。下次给大家介绍这个网站的 `selenium` 爬虫方法，比较麻烦一些，但是作为 `selenium` 练习项目还是不错的，敬请期待吧！