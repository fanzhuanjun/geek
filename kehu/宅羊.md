# 3文件交叉集合

```python
import os

path_xiru = "C:/pwork/宅羊/整理后吸入/"
path_BIS = "C:/pwork/宅羊/整理后BIS/"
path_docx = "C:/pwork/宅羊/docx/"

xiru = [i.split('.')[0][:-2] for i in os.listdir(path_xiru)]
BIS = [i.split('.')[0][:-3] for i in os.listdir(path_BIS)]
docx = [i[:-8] for i in os.listdir(path_docx)]
# os.listdir(path_docx)

len(xiru), len(BIS), len(docx)
from collections import Counter
Counter(xiru)
retA = [i for i in xiru if i in BIS]
xiru
BIS
len(retA)

#求差集，在B中但不在A中
# retD = list(set(listB).difference(set(listA)))
retD = list(set(BIS).difference(set(xiru)))
retD

retD = list(set(docx).difference(set(BIS)))
retD
```



# beta计算

```python
from collections import Counter
from re import X
import pandas as pd
import numpy as np
# from sklearn import linear_model
from sklearn.linear_model import LinearRegression

path = "c:/pwork/宅羊/合并手术轴/0109王兴军0003final.csv"
df = pd.read_csv(path)
df.columns = ['时间', '心率', '平均压', 'AVGBIS', '麻醉时间', '手术时间', '手术30前后', 
'开始至术前30', '术终前30至末']

df['AVGBIS'] = df['AVGBIS'].replace(255, np.nan) # 255缺失值替换
df_drop_0 = df[(df['心率'] != 0) & (df['平均压'] != 0)] # 去掉为0的
df_drop_na = df_drop_0[df_drop_0['心率'].notnull() & df_drop_0['平均压'].notnull() & df_drop_0['AVGBIS'].notnull()]

df_mazui = df_drop_na[df_drop_na['麻醉时间'] == 1]
df_shoushu30 = df_drop_na[df_drop_na['手术30前后'] == 1]
df_shoushu_di30min = df_drop_na[df_drop_na['开始至术前30'] == 1]
df_shoushu_zuihou30min = df_drop_na[df_drop_na['术终前30至末'] == 1]
print(df_mazui.shape, df_shoushu30.shape, df_shoushu_di30min.shape, df_shoushu_zuihou30min.shape)

heartRate_mean = df_mazui['心率'].mean()
meanPressure_mean = df_mazui['平均压'].mean()
avgBIS_mean = df_mazui['AVGBIS'].mean()

df_mazui['心率分箱'] = pd.cut(df_mazui['心率'], bins=[df_mazui['心率'].min(), heartRate_mean*0.8, heartRate_mean*1.2, df_mazui['心率'].max()], labels=['low', 'middle', 'high'])
df_mazui['平均压分箱'] = pd.cut(df_mazui['平均压'], bins=[df_mazui['平均压'].min(), meanPressure_mean*0.8, meanPressure_mean*1.2, df_mazui['平均压'].max()], labels=['low', 'middle', 'high'])
df_mazui['AVGBIS分箱01'] = pd.cut(df_mazui['AVGBIS'], bins=[df_mazui['AVGBIS'].min(), avgBIS_mean*0.8, avgBIS_mean*1.2, df_mazui['AVGBIS'].max()], labels=['low', 'middle', 'high'])
df_mazui['AVGBIS分箱02'] = pd.cut(df_mazui['AVGBIS'], bins=[df_mazui['AVGBIS'].min(), 40, 60, df_mazui['AVGBIS'].max()], labels=['low', 'middle', 'high'])

df_mazui['一致性1'] = (df_mazui['心率分箱'] == df_mazui['AVGBIS分箱01'])
df_mazui['一致性2'] = (df_mazui['心率分箱'] == df_mazui['AVGBIS分箱02'])

df_mazui['一致性3'] = (df_mazui['平均压分箱'] == df_mazui['AVGBIS分箱01'])
df_mazui['一致性4'] = (df_mazui['平均压分箱'] == df_mazui['AVGBIS分箱02'])

df_mazui['一致性5'] = (df_mazui['心率分箱'] == df_mazui['平均压分箱'])

df_mazui['一致性6'] = ((df_mazui['心率分箱'] == df_mazui['平均压分箱']) & (df_mazui['心率分箱'] == df_mazui['AVGBIS分箱01']))
df_mazui['一致性7'] = ((df_mazui['心率分箱'] == df_mazui['平均压分箱']) & (df_mazui['心率分箱'] == df_mazui['AVGBIS分箱02']))

consistency1 = df_mazui['一致性1'].value_counts(normalize=True)[True]
consistency2 = df_mazui['一致性2'].value_counts(normalize=True)[True]
consistency3 = df_mazui['一致性3'].value_counts(normalize=True)[True]
consistency4 = df_mazui['一致性4'].value_counts(normalize=True)[True]
consistency5 = df_mazui['一致性5'].value_counts(normalize=True)[True]
consistency6 = df_mazui['一致性6'].value_counts(normalize=True)[True]
consistency7 = df_mazui['一致性7'].value_counts(normalize=True)[True]

# consistency_mean = np.mean([consistency1, consistency2, consistency3, consistency4, 
#                             consistency5, consistency6, consistency7])
linear_m = LinearRegression()
# (method) fit: (X, y, sample_weight=None) -> LinearRegression
X = np.log(df_mazui[['心率', '平均压']])
y = np.log(df_mazui['AVGBIS'])
linear_m.fit(X, y)
r_2 = linear_m.score(X, y)
beta_xinlv, beta_pingjuya = linear_m.coef_
beta_xinlv, beta_pingjuya

# Counter(df_mazui['一致性7'])

# df_mazui.apply(lambda x: 1 if x.心率分箱 == x.AVGBIS分箱01 else 0)
# df_mazui.iloc[0,:]['心率分箱'] == df_mazui.iloc[0,:]['AVGBIS分箱01']
# df_mazui['心率分箱'].value_counts()
# df_mazui['平均压分箱'].value_counts()
# df_mazui['AVGBIS分箱01'].value_counts()
# df_mazui['AVGBIS分箱02'].value_counts()
# df_mazui['一致性1']

# df[df['麻醉时间'] == 1][['心率', '平均压', 'AVGBIS']].mean()
# df[df['麻醉时间'] == 1][['心率', '平均压', 'AVGBIS']].var()
# df[df['麻醉时间'] == 1][['心率', '平均压', 'AVGBIS']].describe()
# 66.902857 * 0.2
# 68.257143 * 0.2

#  一致性1
# 心率均值 *0.8 ~ 心率均值 * 1.2
# AVGBIS * 0.8 ~ AVGBIS * 1.2
# 3 中情况 一致性 1

#  一致性2
# 心率均值 *0.8 ~ 心率均值 * 1.2
# AVGBIS [40 : 60]
# 3 中情况 一致性 1

#  一致性3
# 平均压 *0.8 ~ 平均压 * 1.2
# AVGBIS * 0.8 ~ AVGBIS * 1.2
# 3 中情况 一致性 1

#  一致性4
# 平均压 *0.8 ~ 平均压 * 1.2
# AVGBIS [40 : 60]
# 3 中情况 一致性 1

#  一致性5
# 心率均值 *0.8 ~ 心率均值 * 1.2
# 平均压 *0.8 ~ 平均压 * 1.2
# 3 中情况 一致性 1

#  一致性6
# 平均压 *0.8 ~ 平均压 * 1.2
# 心率均值 *0.8 ~ 心率均值 * 1.2
# AVGBIS * 0.8 ~ AVGBIS * 1.2

#  一致性7
# 平均压 *0.8 ~ 平均压 * 1.2
# 心率均值 *0.8 ~ 心率均值 * 1.2
# AVGBIS [40 : 60]


# 标准差
# 变异系数
# 心率和平均压 反映 BIS 波动。

# 回归分析

# 7 + beta1 +beta2 

# beta1
```




# BIS数据整理

```python
import pandas as pd
import ast
# path = "C:/pwork/2018,0108方崇彬0001 BIS.xlsx"
# df = pd.read_excel(path, skiprows=2)
# df.columns
# df02 = pd.DataFrame(df.iloc[:, 2])
# df02.to_csv("c:/pwork/pp.txt")
# df = pd.read_csv("c:/pwork/pp.txt", sep='|')
# col = [',Time               ', 'SENSOR  ', 'AVGBIS  ', 'MINBIS  ', 'MAXBIS  ',
#        'AVGBISAL', 'AVGBISA2', 'AVGSQI  ', 'AVGEMG  ', 'AVGSR   ', 'AVGIMPD1',
#        'AVGIMPD2', 'BISBITS ', 'AVGBURST', 'RESVAR  ']
# final_df = df[col]
# final_df.columns = ['Time', 'SENSOR  ', 'AVGBIS  ', 'MINBIS  ', 'MAXBIS  ',
#        'AVGBISAL', 'AVGBISA2', 'AVGSQI  ', 'AVGEMG  ', 'AVGSR   ', 'AVGIMPD1',
#        'AVGIMPD2', 'BISBITS ', 'AVGBURST', 'RESVAR  ']
# final_df.to_csv("")

# final_df['Time'] = final_df['Time'].str.split(',').str[-1]
# final_df

# path.split(".")[0]
# ssd  = "'C:/pwork/500病人所有数据/2018.08/0817朱玖屹0544/0817朱玖屹0544BIS.xls'"
# ssd.split('/')[-1].split(".")[0]

def get_final_data(path):
    my_path = path.split('/')[-1]
    path_name = my_path.split(".")[0]
    path_name_txt = f"c:/pwork/宅羊/废材料/txtdoc/{path_name}.txt"
    df = pd.read_excel(path, skiprows=0)
    df02 = pd.DataFrame(df.iloc[:, 0])
    df02.to_csv(path_name_txt)
    df = pd.read_csv(path_name_txt, sep='|')
    col = [',Time               ', 'SENSOR  ', 'AVGBIS  ', 'MINBIS  ', 'MAXBIS  ',
        'AVGBISAL', 'AVGBISA2', 'AVGSQI  ', 'AVGEMG  ', 'AVGSR   ', 'AVGIMPD1',
        'AVGIMPD2', 'BISBITS ', 'AVGBURST', 'RESVAR  ']
    final_df = df[col]
    final_df.columns = ['Time', 'SENSOR  ', 'AVGBIS  ', 'MINBIS  ', 'MAXBIS  ',
        'AVGBISAL', 'AVGBISA2', 'AVGSQI  ', 'AVGEMG  ', 'AVGSR   ', 'AVGIMPD1',
        'AVGIMPD2', 'BISBITS ', 'AVGBURST', 'RESVAR  ']
    final_df['Time'] = final_df['Time'].str.split(',').str[-1]
    final_df.to_csv(f"c:/pwork/{path_name}.csv")

def write_txt(result):
    if result:
        with open('c:/pwork/errorlog15.txt', 'a', encoding='utf-8') as txtfile:
            txtfile.write(result+'\n')


if __name__ == "__main__":
    ss = "C:/pwork/errorlog14.txt"
    # ss = "C:/pwork/assa.txt"
    
    with open(ss, encoding='utf-8') as a:
        f = a.read()
    # au = ast.literal_eval(f)
    au = [i for i in f.split('\n') if i != '']
    for i in au:
        try:
            get_final_data(i)
            print("done 1!")
        except Exception as e:
            print(e)
            write_txt(i)


# ss = "C:/pwork/errorlog08.txt"
# with open(ss, encoding='utf-8') as a:
#     f = a.read()
#     # au = ast.literal_eval(f)
# au = [i for i in f.split('\n') if i != '']
# au[2]
# ab = [i for i in f.split('\n') if i != '']
# # len(ab)
# path = ab[1]
# my_path = path.split('/')[-1]
# path_name = my_path.split(".")[0]
# path
# path_name_txt = f"c:/pwork/txtdoc/{path_name}.txt"
# df = pd.read_excel(path, skiprows=2)
# df
# df02 = pd.DataFrame(df.iloc[:, 2])
# df02.to_csv(path_name_txt)
# df = pd.read_csv(path_name_txt, sep='|')
# col = [',Time               ', 'SENSOR  ', 'AVGBIS  ', 'MINBIS  ', 'MAXBIS  ',
#     'AVGBISAL', 'AVGBISA2', 'AVGSQI  ', 'AVGEMG  ', 'AVGSR   ', 'AVGIMPD1',
#     'AVGIMPD2', 'BISBITS ', 'AVGBURST', 'RESVAR  ']
# final_df = df[col]
# final_df.columns = ['Time', 'SENSOR  ', 'AVGBIS  ', 'MINBIS  ', 'MAXBIS  ',
#     'AVGBISAL', 'AVGBISA2', 'AVGSQI  ', 'AVGEMG  ', 'AVGSR   ', 'AVGIMPD1',
#     'AVGIMPD2', 'BISBITS ', 'AVGBURST', 'RESVAR  ']
# final_df['Time'] = final_df['Time'].str.split(',').str[-1]
# final_df.to_csv(f"c:/pwork/整理后/{path_name}.csv")

# ss = "C:/pwork/assa.txt"
    
# with open(ss, encoding='utf-8') as a:
#     f = a.read()
# au = ast.literal_eval(f)
# len(au)
# aus = [i for i in au if '.xls' not in i]
# len(aus)
# aus
# ss = "C:/pwork/errorlog01.txt"
# with open(ss, encoding='utf-8') as a:
#     f = a.read()
# au = [i for i in f.split('\n') if i != '']
# au

# sda = "C:/pwork/txtdoc/0301胡运秀0079BIS.txt"
# df = pd.read_csv(sda, sep='|')
# get_final_data("C:/pwork/宅羊/废材料/500病人所有数据可改版/BIS/0702张乐蓉0414BIS.xls")
```




# docx格式转换

```python
import os
from win32com import client as wc #导入模块


base_path = 'C:/pwork/宅羊/500病人所有数据可改版/doc/'
files = [base_path + i for i in os.listdir(base_path)]
target_path = 'C:/pwork/宅羊/500病人所有数据可改版/docx/'

word = wc.Dispatch("Word.Application") # 打开word应用程序
for file in files:
    file_name = target_path + file.split('/')[-1]
    doc = word.Documents.Open(file) #打开word文件
    doc.SaveAs("{}x".format(file_name), 12)#另存为后缀为".docx"的文件，其中参数12指docx文件
    doc.Close() #关闭原来word文件
word.Quit()
print("完成！")


# newpath = "C:/pwork/宅羊/500病人所有数据可改版/docx/0306欧阳东0092CRF.docx"
# f = extract_docx_text(newpath)
# import re
# all_text = re.findall("麻醉开始时间：(.*?)麻醉结束时间：(.*?)手术开始时间：(.*?)手术结束时间：(.*?)主刀医师姓名", f.replace("\n", ""), re.S)
# all_text = [i for i in all_text[0]]
# final_text = [i.replace("|", "").replace(" ", "") for i in all_text]
# final_text

# base_path = 'C:/pwork/宅羊/500病人所有数据可改版/doc/'
# files = [base_path + i for i in os.listdir(base_path)]
# files
# 生成两个轴，对上即可
```




# ols

```python
import pandas as pd
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import PolynomialFeatures 
from matplotlib.style import use
use("ggplot")

import warnings
warnings.filterwarnings('ignore')

def get_fenxi(path, degree=1, y='X（mm）'):
    """
    Args:
        path: 文件路径，如 "C:/client/卢欣奇（在做）/L1点坝体位移（随机补充）.csv"
        degree: 多项式阶数。如degree=2, 则建立二次多项式回归
        y: 因变量。
    """
    # 导入数据，并构建time变量
    df = pd.read_csv(path)
    df['time'] = [i for i in range(1, df.shape[0]+1)]
    # 设定X变量和y变量
    X = df[['time']]
    y = df[y]
    # 假设阶数>=2，则构建多项式。
    if degree >= 2:
        poly = PolynomialFeatures(degree = degree)
        X = poly.fit_transform(X)
    # 添加截距列
    X = sm.add_constant(X)
    # 拟合模型
    mod = sm.OLS(y, X)
    res = mod.fit()
    # 打印结果
    print("--"*15 + f"{path}拟合结果" + "--"*15)
    print(res.summary())
    print("--"*15 + "图标显示如下" + "--"*15 )
    # 作图
#     sns.regplot("time", y, df);plt.show()
    plt.scatter(df['time'], y, color = 'blue')
    plt.plot(df['time'], res.predict(), color = 'red')
    print("--"*15 + "该模型结果显示完毕!" + "--"*15 )

# 实例
# path = "C:/client/卢欣奇（在做）/L1点坝体位移（随机补充）.csv"
# get_fenxi(path=path, degree=2)
```








# 病人大数据分析

```python
import pandas as pd
import numpy as np
# from sklearn import linear_model
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

path_name = ['麻醉时间', '手术30前后', '开始至术前30', '术终前30至末']
paths = ["c:/pwork/宅羊/大数据/"+p+"大数据-均值回归版.csv" for p in path_name]

def get_fenxi(p, type=2, ln=True):
    df = pd.read_csv(p)
    df = df.dropna()
    if ln:
        X = np.log(df[['心率', '平均压']])
        y = np.log(df['AVGBIS'])
    else:
        X = df[['心率', '平均压']]
        y = df['AVGBIS']
    if type == 1:
    # skelarn
        linear_m = LinearRegression().fit(X, y)
        r_2 = linear_m.score(X, y)
        beta_xinlv, beta_pingjuya = linear_m.coef_
        print(f"{p}拟合结果如下: ")
        print("beta1, beta2: ", beta_xinlv, beta_pingjuya)
        print("决定系数R2: ", r_2)
        print("--"*15 + "该模型结果显示完毕!" + "--"*15 )
    # stats
    elif type == 2:
        X = sm.add_constant(X)
        mod = sm.OLS(y, X)
        res = mod.fit()
        print("--"*15 + f"{p}拟合结果" + "--"*15)
        print(res.summary())
        print("--"*15 + "该模型结果显示完毕!" + "--"*15 )

for i in paths:
    get_fenxi(i, type=1, ln=False)

```




# 病人手术时间识别

```python

import pandas as pd
import docx
import re
import os

def extract_docx_text(docFile):
    # 获取文档对象
    document = docx.Document(docFile)
    # 完整的text：
    docx_text = ""
    for para in document.paragraphs:
        docx_text += para.text + '\n'
    return docx_text

def shoushu_date(path):
    f = extract_docx_text(path)

    all_text = re.findall("麻醉开始时间：(.*?)麻醉结束时间：(.*?)手术开始时间：(.*?)手术结束时间：(.*?)主刀医师姓名", f.replace("\n", ""), re.S)
    all_text = [i for i in all_text[0]]
    final_text = [i.replace("|", "").replace(" ", "") for i in all_text]
    return final_text

def yanjiu_date(path):
    document = docx.Document(path)
    table = document.tables[0]
    data = {}
    for row in table.rows:
        text = [cell.text for cell in row.cells]
        text = [i.replace("|", "").replace(" ", "") for i in text]
        data[text[0]] = text[1]
    return data


# docx_path = "c:/pwork/宅羊/docx/"
# paths = [docx_path + i for i in os.listdir(docx_path)]
# for i in paths:
#     print(search_dict(path=i), i)
# paths[4]
def get_shoushu_dummies(path):
    yanjiu_init = yanjiu_date(path)
    shoushu_init = shoushu_date(path)

    yanjiu_init['研究开始日期'] = pd.to_datetime(yanjiu_init['研究开始日期'], format='%Y年%M月%d日')
    # yanjiu_init['研究结束日期'] = pd.to_datetime(yanjiu_init['研究结束日期'], format='%Y年%M月%d日')

    # date_range = pd.date_range(start=yanjiu_init['研究开始日期'], end=yanjiu_init['研究结束日期'])
    date_range = pd.date_range(start=yanjiu_init['研究开始日期'], periods=21)
    date_range = [str(i)[:10] for i in date_range]
    final_date = []
    for i in shoushu_init:
        for date in date_range:
            if i[:2] == date[-2:]:
                final_date.append(date + " " + i[3:])

    final_date = [pd.to_datetime(i, format="%Y-%m-%d %H时%M分") for i in final_date]
    mazui, shoushu = final_date[:2], final_date[2:]
    
    mazui_axis = pd.date_range(start=mazui[0], end=mazui[1], freq='Min')
    shoushu_axis = pd.date_range(start=shoushu[0], end=shoushu[1], freq='Min')
    qianhou30_axis = shoushu_axis[30:-30]


    mazui_feature = pd.Series([1 for i in range(len(mazui_axis))], index=mazui_axis)
    shoushu_feature = pd.Series([1 for i in range(len(shoushu_axis))], index=shoushu_axis)
    shoushu_qianhou30_feature = pd.Series([1 for i in range(len(qianhou30_axis))], index=qianhou30_axis)

    return mazui_feature, shoushu_feature, shoushu_qianhou30_feature

for i in paths:
    try:
        a = get_shoushu_dummies(i)
    except Exception as e:
        print(e)
        print(f"{i}发生错误")

# get_shoushu_dummies("c:/pwork/宅羊/docx/0112徐子良0019CRF.docx")

# f.replace("\n", "")
# text = re.findall("(麻醉开始时间.*?)主刀医师姓名", f.replace("\n", ""), re.S)[0]
# text

# f = extract_docx_text(path)
# f
# all_text = 
# re.findall("术后(.*?)天随访.*?日期(.*?日)", f.replace("\n", ""), re.S)
# all_text = [i for i in all_text[0]]


# data

# keys = None 
# for i, row in enumerate(table.rows): 
#     text = (cell.text for cell in row.cells) 
#     if i == 0: 
#         keys = tuple(text) 
#         continue
#     row_data = dict(zip(keys, text)) 
#     data.append(row_data) 
# print(data)

# # 完整的text：
# paths[10]
# datetime = []
# for i in document.tables[0].rows[:][-2:]:
#     c = i.text
#     datetime.append(c)


# get_shoushu_dummies()

# path = "c:/pwork/宅羊/docx/0112徐子良0019CRF.docx"
# yanjiu_init = yanjiu_date(path)
# shoushu_init = shoushu_date(path)
# yanjiu_init; shoushu_init
# yanjiu_init['研究开始日期'] = pd.to_datetime(yanjiu_init['研究开始日期'], format='%Y年%M月%d日')
# yanjiu_init['研究结束日期'] = pd.to_datetime(yanjiu_init['研究结束日期'], format='%Y年%M月%d日')

# date_range = pd.date_range(start=yanjiu_init['研究开始日期'], end=yanjiu_init['研究结束日期'])
# date_range = pd.date_range(start=yanjiu_init['研究开始日期'], periods=60)
# date_range = [str(i)[:10] for i in date_range]
# final_date = []
# for i in shoushu_init:
#     for date in date_range:
#         if i[:2] == date[-2:]:
#             final_date.append(date + " " + i[3:])
# final_date

# final_date = [pd.to_datetime(i, format="%Y-%m-%d %H时%M分") for i in final_date]
# mazui, shoushu = final_date[:2], final_date[2:]
# print(mazui, shoushu)
# mazui_axis = pd.date_range(start=mazui[0], end=mazui[1], freq='Min')
# shoushu_axis = pd.date_range(start=shoushu[0], end=shoushu[1], freq='Min')
# qianhou30_axis = shoushu_axis[30:-30]


# mazui_feature = pd.Series([1 for i in range(len(mazui_axis))], index=mazui_axis)
# shoushu_feature = pd.Series([1 for i in range(len(shoushu_axis))], index=shoushu_axis)

# return mazui_feature, shoushu_feature, shoushu_qianhou30_feature
```




# 病人手术时间识别修正

```python

import pandas as pd
import docx
import re
import os

def extract_docx_text(docFile):
    # 获取文档对象
    document = docx.Document(docFile)
    # 完整的text：
    docx_text = ""
    for para in document.paragraphs:
        docx_text += para.text + '\n'
    return docx_text

def shoushu_date(path):
    f = extract_docx_text(path)

    all_text = re.findall("麻醉开始时间：(.*?)麻醉结束时间：(.*?)手术开始时间：(.*?)手术结束时间：(.*?)主刀医师姓名", f.replace("\n", ""), re.S)
    all_text = [i for i in all_text[0]]
    final_text = [i.replace("|", "").replace(" ", "") for i in all_text]
    return final_text

def yanjiu_date(path):
    document = docx.Document(path)
    table = document.tables[0]
    data = {}
    for row in table.rows:
        text = [cell.text for cell in row.cells]
        text = [i.replace("|", "").replace(" ", "") for i in text]
        data[text[0]] = text[1]
    return data

# docx_path = "c:/pwork/宅羊/docx/"
# shoushu_date(docx_path+"0817朱玖屹0544CRF.docx")
# yanjiu_init = yanjiu_date(docx_path+"0817朱玖屹0544CRF.docx")
# yanjiu_init['研究开始日期'] = pd.to_datetime(yanjiu_init['研究开始日期'], format='%Y年%m月%d日')
# yanjiu_init['研究结束日期'] = pd.to_datetime(yanjiu_init['研究结束日期'], format='%Y年%M月%d日')
# yanjiu_init
# date_range = pd.date_range(start=yanjiu_init['研究开始日期'], end=yanjiu_init['研究结束日期'])
date_range = pd.date_range(start=yanjiu_init['研究开始日期'], periods=21)
date_range = [str(i)[:10] for i in date_range]
final_date = []
for i in shoushu_init:
    for date in date_range:
        if i[:2] == date[-2:]:
            final_date.append(date + " " + i[3:])
# paths = [docx_path + i for i in os.listdir(docx_path)]
# for i in paths:
#     print(search_dict(path=i), i)
# paths[4]
def get_shoushu_dummies(path):
    yanjiu_init = yanjiu_date(path)
    shoushu_init = shoushu_date(path)

    yanjiu_init['研究开始日期'] = pd.to_datetime(yanjiu_init['研究开始日期'], format='%Y年%M月%d日')
    # yanjiu_init['研究结束日期'] = pd.to_datetime(yanjiu_init['研究结束日期'], format='%Y年%M月%d日')

    # date_range = pd.date_range(start=yanjiu_init['研究开始日期'], end=yanjiu_init['研究结束日期'])
    date_range = pd.date_range(start=yanjiu_init['研究开始日期'], periods=21)
    date_range = [str(i)[:10] for i in date_range]
    final_date = []
    for i in shoushu_init:
        for date in date_range:
            if i[:2] == date[-2:]:
                final_date.append(date + " " + i[3:])

    final_date = [pd.to_datetime(i, format="%Y-%m-%d %H时%M分") for i in final_date]
    mazui, shoushu = final_date[:2], final_date[2:]
    
    mazui_axis = pd.date_range(start=mazui[0], end=mazui[1], freq='Min')
    shoushu_axis = pd.date_range(start=shoushu[0], end=shoushu[1], freq='Min')
    qianhou30_axis = shoushu_axis[30:-30]


    mazui_feature = pd.Series([1 for i in range(len(mazui_axis))], index=mazui_axis)
    shoushu_feature = pd.Series([1 for i in range(len(shoushu_axis))], index=shoushu_axis)
    shoushu_qianhou30_feature = pd.Series([1 for i in range(len(qianhou30_axis))], index=qianhou30_axis)

    return mazui_feature, shoushu_feature, shoushu_qianhou30_feature

for i in paths:
    try:
        a = get_shoushu_dummies(i)
    except Exception as e:
        print(e)
        print(f"{i}发生错误")

# get_shoushu_dummies("c:/pwork/宅羊/docx/0112徐子良0019CRF.docx")

# f.replace("\n", "")
# text = re.findall("(麻醉开始时间.*?)主刀医师姓名", f.replace("\n", ""), re.S)[0]
# text

# f = extract_docx_text(path)
# f
# all_text = 
# re.findall("术后(.*?)天随访.*?日期(.*?日)", f.replace("\n", ""), re.S)
# all_text = [i for i in all_text[0]]


# data

# keys = None 
# for i, row in enumerate(table.rows): 
#     text = (cell.text for cell in row.cells) 
#     if i == 0: 
#         keys = tuple(text) 
#         continue
#     row_data = dict(zip(keys, text)) 
#     data.append(row_data) 
# print(data)

# # 完整的text：
# paths[10]
# datetime = []
# for i in document.tables[0].rows[:][-2:]:
#     c = i.text
#     datetime.append(c)


# get_shoushu_dummies()

# path = "c:/pwork/宅羊/docx/0112徐子良0019CRF.docx"
# yanjiu_init = yanjiu_date(path)
# shoushu_init = shoushu_date(path)
# yanjiu_init; shoushu_init
# yanjiu_init['研究开始日期'] = pd.to_datetime(yanjiu_init['研究开始日期'], format='%Y年%M月%d日')
# yanjiu_init['研究结束日期'] = pd.to_datetime(yanjiu_init['研究结束日期'], format='%Y年%M月%d日')

# date_range = pd.date_range(start=yanjiu_init['研究开始日期'], end=yanjiu_init['研究结束日期'])
# date_range = pd.date_range(start=yanjiu_init['研究开始日期'], periods=60)
# date_range = [str(i)[:10] for i in date_range]
# final_date = []
# for i in shoushu_init:
#     for date in date_range:
#         if i[:2] == date[-2:]:
#             final_date.append(date + " " + i[3:])
# final_date

# final_date = [pd.to_datetime(i, format="%Y-%m-%d %H时%M分") for i in final_date]
# mazui, shoushu = final_date[:2], final_date[2:]
# print(mazui, shoushu)
# mazui_axis = pd.date_range(start=mazui[0], end=mazui[1], freq='Min')
# shoushu_axis = pd.date_range(start=shoushu[0], end=shoushu[1], freq='Min')
# qianhou30_axis = shoushu_axis[30:-30]


# mazui_feature = pd.Series([1 for i in range(len(mazui_axis))], index=mazui_axis)
# shoushu_feature = pd.Series([1 for i in range(len(shoushu_axis))], index=shoushu_axis)

# return mazui_feature, shoushu_feature, shoushu_qianhou30_feature
```




# 病人字典构建

```python
import os
from posix import listdir


path = "C:/pwork/500病人所有数据/"
new_l = [path+i+'/' for i in os.listdir(path)]
sss = [[i+a+'/' for a in os.listdir(i)] for i in new_l]
# sss[0]
d = []
for i in sss:
    d.extend(i)

not (False)
len(d)
d = [i for i in d if not (('退出' in i) or ('放弃' in i))]
# sd = [[i+a+'/' for a in os.listdir(i)] for i in sss]

29+33+89+78+89+80+96+36
d
llll = []
for i in d:
    pas_list =  os.listdir(i)
    for a in pas_list:
        if '吸入.xls' in a:
            llll.append(i+a)
        
llll
len(llll)
for i in d:
    pas_list =  os.listdir(i)
    a = [i for i in pas_list if ('吸入' in i) and ('统计' not in i)]
    if a == []:
        print(i)
    # for a in pas_list:
    #     if 'BIS' in a:
    #         llll.append(i+a)
len(llll)
llll
with open('C:/pwork/宅羊/assa.txt', encoding='utf-8') as a:
    f = a.read()
llll
with open('C:/pwork/assa.txt', 'w', encoding='utf-8') as a:
    a.write(str(llll))

import ast
au = ast.literal_eval(f)
len(au)
auxiru = [i.replace(" BIS", "吸入").replace("xlsx", "xls") for i in au]
auxiru

with open('C:/pwork/宅羊/吸入列表汇总.txt', 'w', encoding='utf-8') as a:
    a.write(str(auxiru))

pd.read_excel(auxiru[0])
```




# 大数据beta计算

```python
from numpy.lib.function_base import _diff_dispatcher
import pandas as pd
import numpy as np
import os
import warnings
warnings.filterwarnings('ignore')

# features = ['麻醉时间', '手术30前后', '开始至术前30', '术终前30至末']
def get_DataFrame(p, feature = '麻醉时间'):
    base_path = "c:/pwork/宅羊/合并手术轴/"
    path_type = "final.csv"
    path = base_path + p + path_type
    df = pd.read_csv(path)
    df.columns = ['时间', '心率', '平均压', 'AVGBIS', '麻醉时间', '手术时间', '手术30前后', 
    '开始至术前30', '术终前30至末']

    df['AVGBIS'] = df['AVGBIS'].replace(255, np.nan) # 255缺失值替换
    df_drop_0 = df[(df['心率'] != 0) & (df['平均压'] != 0)] # 去掉为0的
    df_drop_na = df_drop_0[df_drop_0['心率'].notnull() & df_drop_0['平均压'].notnull() & df_drop_0['AVGBIS'].notnull()]
    df_drop_na['name'] = p
    df_drop_na = df_drop_na[(df_drop_na['心率'] > 40) & (df_drop_na['平均压'] > 40) & (df_drop_na['AVGBIS'] > 20)]
    # 面板数据
    return df_drop_na[df_drop_na[feature] == 1]
    # 个人均值
    # return df_drop_na[df_drop_na[feature] == 1][['心率', '平均压', 'AVGBIS']].mean()

hebing_path = "c:/pwork/宅羊/合并手术轴/"
path_names = [i.split('.')[0][:-5] for i in os.listdir(hebing_path)]

for feature in ['麻醉时间', '手术30前后', '开始至术前30', '术终前30至末', '手术时间'][-1:]:
    def getbigData(p):
        return get_DataFrame(p, feature=feature)
    
    # df = pd.concat(map(getbigData, path_names), axis=0)
    df = pd.DataFrame(map(getbigData, path_names))
    df.to_csv(f"c:/pwork/宅羊/大数据2/{feature}大数据-面板数据.csv", encoding='utf_8_sig')
    print("done 1 !")

print("全部完成！")

```




# 合并手术轴

```python
import pandas as pd
import docx
import re
import os

def extract_docx_text(docFile):
    # 获取文档对象
    document = docx.Document(docFile)
    # 完整的text：
    docx_text = ""
    for para in document.paragraphs:
        docx_text += para.text + '\n'
    return docx_text

def shoushu_date(path):
    f = extract_docx_text(path)

    all_text = re.findall("麻醉开始时间：(.*?)麻醉结束时间：(.*?)手术开始时间：(.*?)手术结束时间：(.*?)主刀医师姓名", f.replace("\n", ""), re.S)
    all_text = [i for i in all_text[0]]
    final_text = [i.replace("|", "").replace(" ", "") for i in all_text]
    return final_text

def yanjiu_date(path):
    document = docx.Document(path)
    table = document.tables[0]
    data = {}
    for row in table.rows:
        text = [cell.text for cell in row.cells]
        text = [i.replace("|", "").replace(" ", "") for i in text]
        data[text[0]] = text[1]
    return data

def get_shoushu_dummies(path):
    yanjiu_init = yanjiu_date(path)
    shoushu_init = shoushu_date(path)

    yanjiu_init['研究开始日期'] = pd.to_datetime(yanjiu_init['研究开始日期'], format='%Y年%m月%d日')
    # yanjiu_init['研究结束日期'] = pd.to_datetime(yanjiu_init['研究结束日期'], format='%Y年%M月%d日')

    # date_range = pd.date_range(start=yanjiu_init['研究开始日期'], end=yanjiu_init['研究结束日期'])
    date_range = pd.date_range(start=yanjiu_init['研究开始日期'], periods=21)
    date_range = [str(i)[:10] for i in date_range]
    final_date = []
    for i in shoushu_init:
        for date in date_range:
            if i[:2] == date[-2:]:
                final_date.append(date + " " + i[3:])

    final_date = [pd.to_datetime(i, format="%Y-%m-%d %H时%M分") for i in final_date]
    mazui, shoushu = final_date[:2], final_date[2:]
    
    mazui_axis = pd.date_range(start=mazui[0], end=mazui[1], freq='Min')
    shoushu_axis = pd.date_range(start=shoushu[0], end=shoushu[1], freq='Min')
    qianhou30_axis = shoushu_axis[30:-30]
    shoushu_di30min_axis = shoushu_axis[30:31]
    shoushu_zuihou30min_axis = shoushu_axis[-31:-30]

    mazui_feature = pd.Series([1 for i in range(len(mazui_axis))], index=mazui_axis, name='麻醉时间')
    shoushu_feature = pd.Series([1 for i in range(len(shoushu_axis))], index=shoushu_axis, name="手术时间")
    shoushu_qianhou30_feature = pd.Series([1 for i in range(len(qianhou30_axis))], index=qianhou30_axis, name="手术30前后")
    shoushu_di30min_feature = pd.Series([1 for i in range(len(shoushu_di30min_axis))], index=shoushu_di30min_axis, name="开始至术前30")
    shoushu_zuihou30min_feature = pd.Series([1 for i in range(len(shoushu_zuihou30min_axis))], index=shoushu_zuihou30min_axis, name="术终前30至末")

    return mazui_feature, shoushu_feature, shoushu_qianhou30_feature, shoushu_di30min_feature, shoushu_zuihou30min_feature


hebing_path = "c:/pwork/宅羊/吸入BIS合并/"
docx_path = "c:/pwork/宅羊/docx/"
target_path = "c:/pwork/宅羊/合并手术轴/"

hebing_type = "合并.csv"
docx_type = "CRF.docx"

list_name = [i.split('/')[-1].split('.')[0][:-2] for i in os.listdir(hebing_path)]
del_list = ['0301王绍兰0076', '0413孔广玺0195', ]

# p = list_name[2]
error_path = []
qiguai_path = []
mazui_error = []
shoushu_error = []
def get_shoushu_zhou_data(p):
    try:
        hebing_data_path = hebing_path + p + hebing_type
        dummis_path = docx_path + p + docx_type

        df_hebing = pd.read_csv(hebing_data_path)
        mazui_feature, shoushu_feature, shoushu_qianhou30_feature, shoushu_di30min_feature, shoushu_zuihou30min_feature = get_shoushu_dummies(dummis_path)
        df_hebing.columns = ['时间', '心率', '平均压', 'AVGBIS']
        df_hebing.set_index("时间", inplace=True)
        df_hebing.index = pd.to_datetime(df_hebing.index)

        # if (mazui_feature.index[0] > df_hebing.index[0]) and (mazui_feature.index[-1] < df_hebing.index[-1]):
        #     pass
        # else:
        #     mazui_error.append(p)
        #     print(p)
        #     print("df_hebing", df_hebing.index[0], df_hebing.index[-1])
        #     print("mazui_feature", mazui_feature.index[0], mazui_feature.index[-1])
        #     print("----")

        # if (shoushu_feature.index[0] > df_hebing.index[0]) and (shoushu_feature.index[-1] < df_hebing.index[-1]):
        #     pass
        # else:
        #     shoushu_error.append(p)
        #     print(p)
        #     print("df_hebing", df_hebing.index[0], df_hebing.index[-1])
        #     print("shoushu_feature", shoushu_feature.index[0], shoushu_feature.index[-1])
        #     print("----")
        # mazui_feature, shoushu_feature, shoushu_qianhou30_feature, shoushu_di30min_feature, shoushu_zuihou30min_feature
        _ = pd.concat((df_hebing, mazui_feature), axis=1)
        _ = pd.concat((_, shoushu_feature), axis=1)
        _ = pd.concat((_, shoushu_qianhou30_feature), axis=1)
        _ = pd.concat((_, shoushu_di30min_feature), axis=1)
        final_data = pd.concat((_, shoushu_zuihou30min_feature), axis=1)
        
        final_data["麻醉时间"].fillna(0, inplace=True)
        final_data["手术时间"].fillna(0, inplace=True)
        final_data["手术30前后"].fillna(0, inplace=True)
        final_data['开始至术前30'] = final_data['开始至术前30'].fillna(method='bfill').fillna(0)
        final_data['术终前30至末'] = final_data['术终前30至末'].fillna(method='ffill').fillna(0)
        # print("麻醉时间", final_data[final_data['麻醉时间'] == 1].shape)
        # print("手术时间", final_data[final_data['手术时间'] == 1].shape)
        # print("手术30前后", final_data[final_data['手术30前后'] == 1].shape)
        # print("---")
        if ((final_data[final_data['麻醉时间'] == 1].shape[0]!=0) and (final_data[final_data['手术时间'] == 1].shape[0]!=0) and (final_data[final_data['手术30前后'] == 1].shape[0]!=0)):
            final_data.to_csv(f"{target_path}{p}final.csv", encoding='utf_8_sig')
        else:
            qiguai_path.append(p)
        
    except Exception as e:
        print(e)
        error_path.append(p)

for i in list_name:
    get_shoushu_zhou_data(i)

error_path
ab = ['0225王小明0066', "0626张家弟0398", "0413霍建华0196"]
get_shoushu_zhou_data()
# len(shoushu_error)
# len(mazui_error)
# qiguai_path
# error_path
# mazui_error
# shoushu_error
# get_shoushu_zhou_data(list_name[0])
# True and False and True

p = list_name[10]
p
hebing_data_path = hebing_path + p + hebing_type
dummis_path = docx_path + p + docx_type

df_hebing = pd.read_csv(hebing_data_path)
mazui_feature, shoushu_feature, shoushu_qianhou30_feature, shoushu_di30min_feature, shoushu_zuihou30min_feature = get_shoushu_dummies(dummis_path)
df_hebing.columns = ['时间', '心率', '平均压', 'AVGBIS']
df_hebing.set_index("时间", inplace=True)
df_hebing.index = pd.to_datetime(df_hebing.index)

# shoushu_di30min_feature
# shoushu_zuihou30min_feature
_ = pd.concat((df_hebing, mazui_feature), axis=1)
_ = pd.concat((_, shoushu_feature), axis=1)
_ = pd.concat((_, shoushu_qianhou30_feature), axis=1)
_ = pd.concat((_, shoushu_di30min_feature), axis=1)
final_data = pd.concat((_, shoushu_zuihou30min_feature), axis=1)

final_data["麻醉时间"].fillna(0, inplace=True)
final_data["手术时间"].fillna(0, inplace=True)
final_data["手术30前后"].fillna(0, inplace=True)
final_data['开始至术前30'] = final_data['开始至术前30'].fillna(method='bfill').fillna(0)
final_data['术终前30至末'] = final_data['术终前30至末'].fillna(method='ffill').fillna(0)
# for pi in qiguai_path:
#     print(pi)
#     get_shoushu_zhou_data(pi)

# qiguai_path
```




# 时间正则表达式

```python
import pandas as pd

path = "C:/pwork/500病人所有数据/2018.01/0109王兴军0003           完成/0109王兴军0003吸入.xls"
path2 = "c:/pwork/整理后/0109王兴军0003 BIS.csv"

df = pd.read_excel(path, skiprows=3)
df2 = pd.read_csv(path2)
df2 = df2[['Time', 'AVGBIS  ']]
df2.columns = ['Time', 'AVGBIS']

# del df2['Unnamed: 0']
# df2['Time']
df["时间"] = pd.to_datetime(df["时间"])
df2['Time'] = pd.to_datetime(df2['Time'], format='%m/%d/%Y %H:%M:%S')

df.set_index("时间", inplace=True)
df2.set_index("Time", inplace=True)

df01 = df[~df['平均压'].isnull()]

print(df01.shape, df2.shape)
# pd.merge(df, df2)
final = pd.concat([df01, df2], axis=1)
final.to_csv("c:/pwork/王兴军.csv", encoding='utf_8_sig')

path3 = "C:/pwork/500病人所有数据/2018.01/0109王兴军0003           完成/20180109王兴军CRF表 0003.docx"
# with open(path) as a:
#     f = a.read()

import docx
document = docx.Document(path3)
# s.tables[0]
# tables = document.tables #获取文件中的表格集
def extract_docx_text(docFile):
    # 获取文档对象
    document = docx.Document(docFile)
    # 完整的text：
    docx_text = ""
    for para in document.paragraphs:
        docx_text += para.text + '\n'
    return docx_text

import re
f = extract_docx_text(path3)
"麻醉开始时间" in f

my_text = "麻醉开始时间oihaosdhasd主刀医师姓名"
mh = re.compile(my_text)
s = re.match("麻醉(.*)姓名", f, re.S)
s.group(0)

f.replace("\n", "")
text = re.findall("(麻醉开始时间.*?)主刀医师姓名", f.replace("\n", ""), re.S)[0]
text

all_text = re.findall("麻醉开始时间：(.*?)麻醉结束时间：(.*?)手术开始时间：(.*?)手术结束时间：(.*?)主刀医师姓名", f.replace("\n", ""), re.S)
all_text = [i for i in all_text[0]]

[i.replace("|", "").replace(" ", "") for i in all_text]

```




# 所有人的一致性和Beta

```python
# from typing import final
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import os
import warnings
warnings.filterwarnings('ignore')
error_log = []

def get_params(df_target):
    heartRate_mean = df_target['心率'].mean()
    meanPressure_mean = df_target['平均压'].mean()
    avgBIS_mean = df_target['AVGBIS'].mean()

    df_target['心率分箱'] = pd.cut(df_target['心率'], bins=[0, heartRate_mean*0.8, heartRate_mean*1.2, 10000], labels=['low', 'middle', 'high'])
    df_target['平均压分箱'] = pd.cut(df_target['平均压'], bins=[0, meanPressure_mean*0.8, meanPressure_mean*1.2, 10000], labels=['low', 'middle', 'high'])
    df_target['AVGBIS分箱01'] = pd.cut(df_target['AVGBIS'], bins=[0, avgBIS_mean*0.8, avgBIS_mean*1.2, 10000], labels=['low', 'middle', 'high'])
    df_target['AVGBIS分箱02'] = pd.cut(df_target['AVGBIS'], bins=[0, 40, 60, 10000], labels=['low', 'middle', 'high'])

    df_target['一致性1'] = (df_target['心率分箱'] == df_target['AVGBIS分箱01'])
    df_target['一致性2'] = (df_target['心率分箱'] == df_target['AVGBIS分箱02'])

    df_target['一致性3'] = (df_target['平均压分箱'] == df_target['AVGBIS分箱01'])
    df_target['一致性4'] = (df_target['平均压分箱'] == df_target['AVGBIS分箱02'])

    df_target['一致性5'] = (df_target['心率分箱'] == df_target['平均压分箱'])

    df_target['一致性6'] = ((df_target['心率分箱'] == df_target['平均压分箱']) & (df_target['心率分箱'] == df_target['AVGBIS分箱01']))
    df_target['一致性7'] = ((df_target['心率分箱'] == df_target['平均压分箱']) & (df_target['心率分箱'] == df_target['AVGBIS分箱02']))
    
    consistency1 = df_target['一致性1'].value_counts(normalize=True)[True] if True in list(df_target['一致性1'].value_counts(normalize=True).index) else 0
    consistency2 = df_target['一致性2'].value_counts(normalize=True)[True] if True in list(df_target['一致性2'].value_counts(normalize=True).index) else 0
    consistency3 = df_target['一致性3'].value_counts(normalize=True)[True] if True in list(df_target['一致性3'].value_counts(normalize=True).index) else 0
    consistency4 = df_target['一致性4'].value_counts(normalize=True)[True] if True in list(df_target['一致性4'].value_counts(normalize=True).index) else 0
    consistency5 = df_target['一致性5'].value_counts(normalize=True)[True] if True in list(df_target['一致性5'].value_counts(normalize=True).index) else 0
    consistency6 = df_target['一致性6'].value_counts(normalize=True)[True] if True in list(df_target['一致性6'].value_counts(normalize=True).index) else 0
    consistency7 = df_target['一致性7'].value_counts(normalize=True)[True] if True in list(df_target['一致性7'].value_counts(normalize=True).index) else 0

    linear_m = LinearRegression()
    X = np.log(df_target[['心率', '平均压']])
    y = np.log(df_target['AVGBIS'])
    linear_m.fit(X, y)
    r_2 = linear_m.score(X, y)
    beta_xinlv, beta_pingjuya = linear_m.coef_
    
    return [consistency1, consistency2, consistency3, consistency4, consistency5, consistency6, consistency7, beta_xinlv, beta_pingjuya, r_2]

def get_table(p):
    final_list= [p]
    base_path = "c:/pwork/宅羊/合并手术轴/"
    path_type = "final.csv"
    path = base_path + p + path_type
    df = pd.read_csv(path)
    df.columns = ['时间', '心率', '平均压', 'AVGBIS', '麻醉时间', '手术时间', '手术30前后', 
    '开始至术前30', '术终前30至末']

    df['AVGBIS'] = df['AVGBIS'].replace(255, np.nan) # 255缺失值替换
    df_drop_0 = df[(df['心率'] != 0) & (df['平均压'] != 0)] # 去掉为0的
    # 去掉缺失值
    df_drop_na = df_drop_0[df_drop_0['心率'].notnull() & df_drop_0['平均压'].notnull() & df_drop_0['AVGBIS'].notnull()]
    # 筛选出满足条件的数据（心率>40, 平均压>40, BIS>20）
    df_drop_na = df_drop_na[(df_drop_na['心率'] > 40) & (df_drop_na['平均压'] > 40) & (df_drop_na['AVGBIS'] > 20)]

    df_mazui = df_drop_na[df_drop_na['麻醉时间'] == 1]
    df_shoushu30 = df_drop_na[df_drop_na['手术30前后'] == 1]
    df_shoushu_di30min = df_drop_na[df_drop_na['开始至术前30'] == 1]
    df_shoushu_zuihou30min = df_drop_na[df_drop_na['术终前30至末'] == 1]
    df_shoushu = df_drop_na[df_drop_na['手术时间'] == 1]
    # print(df_mazui.shape, df_shoushu30.shape, df_shoushu_di30min.shape, df_shoushu_zuihou30min.shape)
    # print(df_mazui.isnull().sum(), df_shoushu30.isnull().sum(), df_shoushu_di30min.isnull().sum(), df_shoushu_zuihou30min.isnull().sum())
    for i in [df_mazui, df_shoushu30, df_shoushu_di30min, df_shoushu_zuihou30min, df_shoushu]:
        if i.shape[0] == 0:
            final_list.extend([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan])
            error_log.append(p)
        else:
            params = get_params(i)
            final_list.extend(params)
    final_series = pd.Series(final_list, index=[
        "name", 
        "1_consistency1", "1_consistency2", "1_consistency3", "1_consistency4", "1_consistency5", "1_consistency6", "1_consistency7", "1_beta_xinlv", "1_beta_pingjuya", "1_r_2",
        "2_consistency1", "2_consistency2", "2_consistency3", "2_consistency4", "2_consistency5", "2_consistency6", "2_consistency7", "2_beta_xinlv", "2_beta_pingjuya", "2_r_2",
        "3_consistency1", "3_consistency2", "3_consistency3", "3_consistency4", "3_consistency5", "3_consistency6", "3_consistency7", "3_beta_xinlv", "3_beta_pingjuya", "3_r_2",
        "4_consistency1", "4_consistency2", "4_consistency3", "4_consistency4", "4_consistency5", "4_consistency6", "4_consistency7", "4_beta_xinlv", "4_beta_pingjuya", "4_r_2",
        "5_consistency1", "5_consistency2", "5_consistency3", "5_consistency4", "5_consistency5", "5_consistency6", "5_consistency7", "5_beta_xinlv", "5_beta_pingjuya", "5_r_2",
    ])
    return final_series

hebing_path = "c:/pwork/宅羊/合并手术轴/"
path_names = [i.split('.')[0][:-5] for i in os.listdir(hebing_path)]

# for i in path_names:
    # get_table(i)"4_consistency1", "4_consistency2", "4_consistency3", "4_consistency4", "4_consistency5", "4_consistency6", "4_consistency7", "4_beta_xinlv", "4_beta_pingjuya", "4_r_2",
# a = get_table(path_names[3])
# b = get_table(path_names[4])
# get_table('0225王小明0066')
# pd.DataFrame([a, b])

df = pd.DataFrame(map(get_table, path_names))
# error_log
df.to_csv('c:/pwork/宅羊/大数据2/所有人的一致性和Beta.csv', encoding='utf_8_sig')
# get_params(path_names[0])

# debug
# p = path_names[4]
# final_list= []
# base_path = "c:/pwork/宅羊/合并手术轴/"
# path_type = "final.csv"
# path = base_path + p + path_type
# df = pd.read_csv(path)
# df.columns = ['时间', '心率', '平均压', 'AVGBIS', '麻醉时间', '手术时间', '手术30前后', 
# '开始至术前30', '术终前30至末']

# df['AVGBIS'] = df['AVGBIS'].replace(255, np.nan) # 255缺失值替换
# df_drop_0 = df[(df['心率'] != 0) & (df['平均压'] != 0)] # 去掉为0的
# df_drop_na = df_drop_0[df_drop_0['心率'].notnull() & df_drop_0['平均压'].notnull() & df_drop_0['AVGBIS'].notnull()]

# df_mazui = df_drop_na[df_drop_na['麻醉时间'] == 1]
# df_shoushu30 = df_drop_na[df_drop_na['手术30前后'] == 1]
# df_shoushu_di30min = df_drop_na[df_drop_na['开始至术前30'] == 1]
# df_shoushu_zuihou30min = df_drop_na[df_drop_na['术终前30至末'] == 1]

# # get Params
# df_target = df_shoushu_zuihou30min
# heartRate_mean = df_target['心率'].mean()
# meanPressure_mean = df_target['平均压'].mean()
# avgBIS_mean = df_target['AVGBIS'].mean()


# df_target['心率分箱'] = pd.cut(df_target['心率'], bins=[0, heartRate_mean*0.8, heartRate_mean*1.2, 10000], labels=['low', 'middle', 'high'])
# [0, heartRate_mean*0.8, heartRate_mean*1.2, 10000]
# df_target['平均压分箱'] = pd.cut(df_target['平均压'], bins=[0, meanPressure_mean*0.8, meanPressure_mean*1.2, 10000], labels=['low', 'middle', 'high'])
# df_target['AVGBIS分箱01'] = pd.cut(df_target['AVGBIS'], bins=[0, avgBIS_mean*0.8, avgBIS_mean*1.2, 10000], labels=['low', 'middle', 'high'])
# df_target['AVGBIS分箱02'] = pd.cut(df_target['AVGBIS'], bins=[0, 40, 60, 10000], labels=['low', 'middle', 'high'])

# df_target['一致性1'] = (df_target['心率分箱'] == df_target['AVGBIS分箱01'])
# df_target['一致性2'] = (df_target['心率分箱'] == df_target['AVGBIS分箱02'])

# df_target['一致性3'] = (df_target['平均压分箱'] == df_target['AVGBIS分箱01'])
# df_target['一致性4'] = (df_target['平均压分箱'] == df_target['AVGBIS分箱02'])

# df_target['一致性5'] = (df_target['心率分箱'] == df_target['平均压分箱'])

# df_target['一致性6'] = ((df_target['心率分箱'] == df_target['平均压分箱']) & (df_target['心率分箱'] == df_target['AVGBIS分箱01']))
# df_target['一致性7'] = ((df_target['心率分箱'] == df_target['平均压分箱']) & (df_target['心率分箱'] == df_target['AVGBIS分箱02']))

# consistency1 = df_target['一致性1'].value_counts(normalize=True)[True]
# consistency2 = df_target['一致性2'].value_counts(normalize=True)[True]
# consistency3 = df_target['一致性3'].value_counts(normalize=True)[True]
# consistency4 = df_target['一致性4'].value_counts(normalize=True)[True]
# consistency5 = df_target['一致性5'].value_counts(normalize=True)[True]
# consistency6 = df_target['一致性6'].value_counts(normalize=True)[True]
# consistency7 = df_target['一致性7'].value_counts(normalize=True)[True]

# linear_m = LinearRegression()
# X = np.log(df_target[['心率', '平均压']])
# y = np.log(df_target['AVGBIS'])
# linear_m.fit(X, y)
# r_2 = linear_m.score(X, y)
# beta_xinlv, beta_pingjuya = linear_m.coef_

# de = pd.read_csv("c:/pwork/宅羊/大数据/所有人的一致性和Beta.csv")
# # de.dropna(axis=0, how='all')
# de0 = de[['1_consistency1', '1_consistency2',
#        '1_consistency3', '1_consistency4', '1_consistency5', '1_consistency6',
#        '1_consistency7', '1_beta_xinlv', '1_beta_pingjuya', '1_r_2',
#        '2_consistency1', '2_consistency2', '2_consistency3', '2_consistency4',
#        '2_consistency5', '2_consistency6', '2_consistency7', '2_beta_xinlv',
#        '2_beta_pingjuya', '2_r_2', '3_consistency1', '3_consistency2',
#        '3_consistency3', '3_consistency4', '3_consistency5', '3_consistency6',
#        '3_consistency7', '3_beta_xinlv', '3_beta_pingjuya', '3_r_2',
#        '4_consistency1', '4_consistency2', '4_consistency3', '4_consistency4',
#        '4_consistency5', '4_consistency6', '4_consistency7', '4_beta_xinlv',
#        '4_beta_pingjuya', '4_r_2']]
# de0.dropna(how="all")
```




# 吸入BIS合并

```python
import pandas as pd
import os

xiru_path = "c:/pwork/宅羊/整理后吸入二改版/"
BIS_path = "c:/pwork/宅羊/整理后BIS/"
target_path = "c:/pwork/宅羊/吸入BIS合并/"
xiru_type = "吸入.csv"
bis_type = "BIS.csv"

list_name = [i.split('/')[-1].split('.')[0][:-2] for i in os.listdir(xiru_path)]

# p = list_name[0]
error_name = []
def get_concat_data(p):
    try:
        xiru_data_path = xiru_path + p + xiru_type
        bis_data_path = BIS_path + p + bis_type
        df_xiru = pd.read_csv(xiru_data_path)
        df_BIS = pd.read_csv(bis_data_path)

        df_xiru['时间'] = pd.to_datetime(df_xiru['时间'])
        df_BIS['Time'] = pd.to_datetime(df_BIS['Time'])

        df_BIS_AVG = df_BIS[['Time', 'AVGBIS  ']]
        df_BIS_AVG.columns = ['Time', 'AVGBIS']
        df_BIS_AVG = df_BIS_AVG.drop_duplicates(subset='Time', keep='first')
        
        df_xiru.set_index('时间', inplace=True)
        df_BIS_AVG.set_index('Time', inplace=True)

        final_df = pd.concat([df_xiru, df_BIS_AVG], axis=1)
        del final_df['Unnamed: 0']
        # print(final_df)
        final_df.to_csv(target_path+p+"合并.csv", encoding='utf_8_sig')
    except Exception as e:
        print(e)
        error_name.append(p)

for pa in list_name:
    get_concat_data(pa)

len(error_name)
# get_concat_data(p)
# p = error_name[4]
# p
# xiru_data_path = xiru_path + p + xiru_type
# bis_data_path = BIS_path + p + bis_type
# df_xiru = pd.read_csv(xiru_data_path)
# df_BIS = pd.read_csv(bis_data_path)

# df_xiru['时间'] = pd.to_datetime(df_xiru['时间'])
# df_BIS['Time'] = pd.to_datetime(df_BIS['Time'])

# df_BIS_AVG = df_BIS[['Time', 'AVGBIS  ']]
# df_BIS_AVG.columns = ['Time', 'AVGBIS']
# df_BIS_AVG.drop_duplicates(subset='Time', keep='first')
# # from collections import Counter
# # Counter(df_BIS['Time'])


# df_xiru.set_index('时间', inplace=True)
# df_BIS_AVG.set_index('Time', inplace=True)
# df_xiru
# df_BIS_AVG

# # pd.merge(df_xiru, df_BIS_AVG)
# final_df = pd.concat([df_xiru, df_BIS_AVG], axis=1)
# del final_df['Unnamed: 0']
# # print(final_df)
# final_df.to_csv(target_path+p+"合并.csv", encoding='utf_8_sig')
# 一致性1	一致性2	一致性3	一致性4	一致性5	一致性6	一致性7	beta1	beta2
```




# 吸入表格整理

```python
import pandas as pd
import os

base_path = "c:/pwork/宅羊/整理后吸入/"
target_path = "c:/pwork/宅羊/整理后吸入二改版/"
paths = [base_path + i for i in os.listdir(base_path)]

wu_xinlv = []
for p in paths:
    name = p.split("/")[-1].split('.')[0]
    try:
        df = pd.read_excel(p, skiprows=3)
        a = df[['时间', '心率', '平均压']]
        a = a[~a['时间'].isnull()]
        final_data = a.drop_duplicates(subset='时间', keep='first')
        final_data.to_csv(f"{target_path}{name}.csv", encoding='utf_8_sig')
    except Exception as e:
        wu_xinlv.append(p)
len(wu_xinlv)

wu_HeartRate = []
for p in wu_xinlv:
    name = p.split("/")[-1].split('.')[0]
    try:
        df = pd.read_excel(p, skiprows=3)
        a = df[['时间', 'HeartRate', 'MeanPressure']]
        a.columns = ['时间', '心率', '平均压']
        a = a[~a['时间'].isnull()]
        final_data = a.drop_duplicates(subset='时间', keep='first')
        final_data.to_csv(f"{target_path}{name}.csv", encoding='utf_8_sig')
    except Exception as e:
        wu_HeartRate.append(p)

len(wu_HeartRate)
wu_HeartRate
# wu_HeartRate

# wu_pingjunya = []
# for p in paths:
#     try:
#         df = pd.read_excel(p, skiprows=3)
#         a = df['平均压']
#     except Exception as e:
#         wu_pingjunya.append(p)
# len(wu_pingjunya) # 281

# # df = pd.read_excel(paths[0], skiprows=3)
# # df['平均压']

# wu_MeanPressure = []
# for p in wu_pingjunya:
#     try:
#         df = pd.read_excel(p, skiprows=3)
#         a = df['MeanPressure']
#     except Exception as e:
#         wu_MeanPressure.append(p)
# len(wu_MeanPressure)

# wu_MeanPressure

# pe = 'c:/pwork/宅羊/整理后吸入/0116邱蓉0024吸入.xls'
# df = pd.read_excel(pe, skiprows=3)
# a = df[['时间', '心率', '平均压']]
# a[~a['时间'].isnull()]
# a.tail(15)
```




# 一致性分析


```python
import pandas as pd
import numpy as np
import os
import warnings
warnings.filterwarnings('ignore')

path = "c:/pwork/宅羊/大数据/所有人的一致性和Beta.csv"
df = pd.read_csv(path)
del df['Unnamed: 0']
df.set_index("name", inplace=True)
df.describe().T.round(3).to_csv("一致性表描述统计.csv", encoding='utf_8_sig')

df.corr()
```



