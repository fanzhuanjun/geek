```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from dateutil.relativedelta import relativedelta
import numpy as np
import warnings
warnings.simplefilter('ignore')
```


```python
df_sale = pd.read_csv("C:/pwork/重要文件cleaningData.csv")
```


```python
del df_sale['Unnamed: 0']
```


```python
df_sale.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sum_Sales Units</th>
      <th>print_dum</th>
      <th>Looseleaf_dum</th>
      <th>ebook_dum</th>
      <th>mix_dum</th>
      <th>total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>698239.000000</td>
      <td>698239.000000</td>
      <td>698239.000000</td>
      <td>698239.000000</td>
      <td>698239.000000</td>
      <td>698239.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>9.047070</td>
      <td>0.133121</td>
      <td>0.015298</td>
      <td>0.188358</td>
      <td>0.141094</td>
      <td>0.286276</td>
    </tr>
    <tr>
      <th>std</th>
      <td>104.552096</td>
      <td>0.339705</td>
      <td>0.122737</td>
      <td>0.390998</td>
      <td>0.348118</td>
      <td>0.452020</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-3469.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>23609.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_sale.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 698239 entries, 0 to 698238
    Data columns (total 9 columns):
    New Wiley Fice Code    698239 non-null object
    Product Family Name    698239 non-null object
    Sum_Sales Units        698239 non-null int64
    CYCM                   698239 non-null object
    print_dum              698239 non-null float64
    Looseleaf_dum          698239 non-null float64
    ebook_dum              698239 non-null float64
    mix_dum                698239 non-null float64
    total                  698239 non-null float64
    dtypes: float64(5), int64(1), object(3)
    memory usage: 47.9+ MB
    

# 1. Linear Regression (sale~print+ebook+Looseleaf, All data)


```python
X = df_sale[['print_dum', 'ebook_dum', 'Looseleaf_dum']]
y = df_sale['Sum_Sales Units']
```


```python
import statsmodels.api as sm

def get_OLS(X, y):
    X = sm.add_constant(X)
    mod = sm.OLS(y, X)
    res = mod.fit()
    return res.summary()
```


```python
get_OLS(X, y)
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>     <td>Sum_Sales Units</td> <th>  R-squared:         </th>  <td>   0.002</td>  
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.002</td>  
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   548.7</td>  
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 13 Jan 2021</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   
</tr>
<tr>
  <th>Time:</th>                 <td>14:21:32</td>     <th>  Log-Likelihood:    </th> <td>-4.2365e+06</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>698239</td>      <th>  AIC:               </th>  <td>8.473e+06</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>698235</td>      <th>  BIC:               </th>  <td>8.473e+06</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>     
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     
</tr>
</table>
<table class="simpletable">
<tr>
        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>         <td>    8.3248</td> <td>    0.145</td> <td>   57.430</td> <td> 0.000</td> <td>    8.041</td> <td>    8.609</td>
</tr>
<tr>
  <th>print_dum</th>     <td>   12.3944</td> <td>    0.374</td> <td>   33.155</td> <td> 0.000</td> <td>   11.662</td> <td>   13.127</td>
</tr>
<tr>
  <th>ebook_dum</th>     <td>   -6.1446</td> <td>    0.323</td> <td>  -19.042</td> <td> 0.000</td> <td>   -6.777</td> <td>   -5.512</td>
</tr>
<tr>
  <th>Looseleaf_dum</th> <td>   15.0166</td> <td>    1.027</td> <td>   14.617</td> <td> 0.000</td> <td>   13.003</td> <td>   17.030</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>2772869.475</td> <th>  Durbin-Watson:     </th>     <td>   0.895</td>     
</tr>
<tr>
  <th>Prob(Omnibus):</th>   <td> 0.000</td>    <th>  Jarque-Bera (JB):  </th> <td>9418336352644.680</td>
</tr>
<tr>
  <th>Skew:</th>            <td>99.734</td>    <th>  Prob(JB):          </th>     <td>    0.00</td>     
</tr>
<tr>
  <th>Kurtosis:</th>       <td>17994.359</td>  <th>  Cond. No.          </th>     <td>    8.48</td>     
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.



# 2. Linear Regression (sale~print+ebook+Looseleaf, Only include 30% large customers )


```python
code_sale = df_sale.groupby('New Wiley Fice Code')['Sum_Sales Units'].sum()

```


```python
q_sale = pd.qcut(code_sale, 3, labels=['low', 'middle', 'high'])
```


```python
code_list = list(q_sale[q_sale == 'high'].index)
```


```python
df_sale_high = df_sale[df_sale['New Wiley Fice Code'].isin(code_list)]

X1 = df_sale_high[['print_dum', 'ebook_dum', 'Looseleaf_dum']]
y1 = df_sale_high['Sum_Sales Units']
```


```python
get_OLS(X1, y1)
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>     <td>Sum_Sales Units</td> <th>  R-squared:         </th>  <td>   0.002</td>  
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.002</td>  
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   522.2</td>  
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 13 Jan 2021</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   
</tr>
<tr>
  <th>Time:</th>                 <td>14:21:40</td>     <th>  Log-Likelihood:    </th> <td>-4.0136e+06</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>658332</td>      <th>  AIC:               </th>  <td>8.027e+06</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>658328</td>      <th>  BIC:               </th>  <td>8.027e+06</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>     
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     
</tr>
</table>
<table class="simpletable">
<tr>
        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>         <td>    8.8144</td> <td>    0.154</td> <td>   57.240</td> <td> 0.000</td> <td>    8.513</td> <td>    9.116</td>
</tr>
<tr>
  <th>print_dum</th>     <td>   12.6083</td> <td>    0.392</td> <td>   32.160</td> <td> 0.000</td> <td>   11.840</td> <td>   13.377</td>
</tr>
<tr>
  <th>ebook_dum</th>     <td>   -6.5037</td> <td>    0.341</td> <td>  -19.046</td> <td> 0.000</td> <td>   -7.173</td> <td>   -5.834</td>
</tr>
<tr>
  <th>Looseleaf_dum</th> <td>   15.3236</td> <td>    1.076</td> <td>   14.243</td> <td> 0.000</td> <td>   13.215</td> <td>   17.432</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>2590557.925</td> <th>  Durbin-Watson:     </th>     <td>   0.895</td>     
</tr>
<tr>
  <th>Prob(Omnibus):</th>   <td> 0.000</td>    <th>  Jarque-Bera (JB):  </th> <td>7907351275402.039</td>
</tr>
<tr>
  <th>Skew:</th>            <td>96.897</td>    <th>  Prob(JB):          </th>     <td>    0.00</td>     
</tr>
<tr>
  <th>Kurtosis:</th>       <td>16980.368</td>  <th>  Cond. No.          </th>     <td>    8.39</td>     
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.




```python
# df_sale_high.to_csv("c:/pwork/sale_high.csv", encoding='utf-8')
```

# 3. Linear Regression (sale~print+ebook+Looseleaf + trend + season, Only include 30% large customers )


```python
df_sale_high = df_sale[df_sale['New Wiley Fice Code'].isin(code_list)]
df_sale_high['CYCM'] = pd.to_datetime(df_sale_high['CYCM'])
df_sale_high['year'] = df_sale_high['CYCM'].dt.year
df_sale_high['month'] = df_sale_high['CYCM'].dt.month
df_sale_high['year'] = df_sale_high['year'].astype('str')
df_sale_high['month'] = df_sale_high['month'].astype('str')

X2 = df_sale_high[['print_dum', 'ebook_dum', 'Looseleaf_dum', 'year', 'month']]
X2 = pd.get_dummies(X1, drop_first=True)

y2 = df_sale_high['Sum_Sales Units']
```


```python
get_OLS(X2, y2)
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>     <td>Sum_Sales Units</td> <th>  R-squared:         </th>  <td>   0.002</td>  
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.002</td>  
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   522.2</td>  
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 13 Jan 2021</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   
</tr>
<tr>
  <th>Time:</th>                 <td>14:21:46</td>     <th>  Log-Likelihood:    </th> <td>-4.0136e+06</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>658332</td>      <th>  AIC:               </th>  <td>8.027e+06</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>658328</td>      <th>  BIC:               </th>  <td>8.027e+06</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>      <td> </td>     
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     
</tr>
</table>
<table class="simpletable">
<tr>
        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>         <td>    8.8144</td> <td>    0.154</td> <td>   57.240</td> <td> 0.000</td> <td>    8.513</td> <td>    9.116</td>
</tr>
<tr>
  <th>print_dum</th>     <td>   12.6083</td> <td>    0.392</td> <td>   32.160</td> <td> 0.000</td> <td>   11.840</td> <td>   13.377</td>
</tr>
<tr>
  <th>ebook_dum</th>     <td>   -6.5037</td> <td>    0.341</td> <td>  -19.046</td> <td> 0.000</td> <td>   -7.173</td> <td>   -5.834</td>
</tr>
<tr>
  <th>Looseleaf_dum</th> <td>   15.3236</td> <td>    1.076</td> <td>   14.243</td> <td> 0.000</td> <td>   13.215</td> <td>   17.432</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>2590557.925</td> <th>  Durbin-Watson:     </th>     <td>   0.895</td>     
</tr>
<tr>
  <th>Prob(Omnibus):</th>   <td> 0.000</td>    <th>  Jarque-Bera (JB):  </th> <td>7907351275402.039</td>
</tr>
<tr>
  <th>Skew:</th>            <td>96.897</td>    <th>  Prob(JB):          </th>     <td>    0.00</td>     
</tr>
<tr>
  <th>Kurtosis:</th>       <td>16980.368</td>  <th>  Cond. No.          </th>     <td>    8.39</td>     
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.



# 4. Linear Regression (sale~print+ebook+Looseleaf + trend + season + customersID, Only include Top 500 customers)


```python
new_codes = df_sale.groupby('New Wiley Fice Code')['Sum_Sales Units'].sum().sort_values(ascending=False)
top500_codes = list(new_codes.index[:500])
```


```python
df_sale_500 = df_sale[df_sale['New Wiley Fice Code'].isin(top500_codes)]
df_sale_500['CYCM'] = pd.to_datetime(df_sale_500['CYCM'])
df_sale_500['year'] = df_sale_500['CYCM'].dt.year
df_sale_500['month'] = df_sale_500['CYCM'].dt.month
df_sale_500['year'] = df_sale_500['year'].astype('str')
df_sale_500['month'] = df_sale_500['month'].astype('str')

X3 = df_sale_500[['print_dum', 'ebook_dum', 'Looseleaf_dum', 'year', 'month', 'New Wiley Fice Code']]
X3 = pd.get_dummies(X3, drop_first=True)

y3 = df_sale_500['Sum_Sales Units']
```


```python
from sklearn.linear_model import LinearRegression

linear_m = LinearRegression().fit(X3, y3)
r_2 = linear_m.score(X3, y3)
# print_dum_beta, ebook_dum_beta, Looseleaf_dum_beta = linear_m.coef_[:3]
```


```python
r_2
```




    0.04746824695516727




```python
print_dum_beta, ebook_dum_beta, Looseleaf_dum_beta = linear_m.coef_[:3]
print_dum_beta, ebook_dum_beta, Looseleaf_dum_beta
```




    (18.944374013889895, -3.878368194500684, 23.774940242262954)




```python

```
